I"$N<h1 id="文章汇总">文章汇总</h1>
<p>在这里，我将近期读过的一些文章做一个汇总与梳理。之后这篇汇总涉及到的篇目将随着阅读的深入不断的增加。
这里包括的文章大多是与分布式深度神经网络训练相关的，不过这个领域包括许多细分的方向，各篇文章侧重的重点有所不同，当然不同方向的文章的关注点也有交集。为了更好的整理这个目录性质的文章汇总，我依据个人理解粗略对文章进行分类，并放入相应的类别下。某些文章如果与多个主题相关，它们将会在多个类别下出现。</p>

<p>[TOC]</p>

<h2 id="综述">综述</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: center">论文名称</th>
      <th style="text-align: center">发表刊物</th>
      <th style="text-align: center">发表时间</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1802.09941">Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-02</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1810.11787">A Hitchhiker’s Guide On Distributed Training of Deep Neural Networks</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-10</td>
    </tr>
  </tbody>
</table>

<h2 id="large-batch-training">Large Batch Training</h2>
<h3 id="系统整体设计">系统整体设计</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">论文名称</th>
      <th style="text-align: center">发表刊物</th>
      <th style="text-align: center">发表时间</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://arxiv.org/abs/1706.02677">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2017-06</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://dl.acm.org/citation.cfm?id=3225069">ImageNet Training in Minutes</a></td>
      <td style="text-align: center">ICPP 2018</td>
      <td style="text-align: center">2018-08</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://arxiv.org/abs/1711.04325">Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2017-11</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://arxiv.org/abs/1807.11205">Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-07</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1902.06855">Optimizing Network Performance for Distributed DNN Training on GPU Clusters: ImageNet/AlexNet Training in 1.5 Minutes</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2019-02</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1810.01993">Exascale Deep Learning for Climate Analytics</a></td>
      <td style="text-align: center">SC 19</td>
      <td style="text-align: center">2018-10</td>
    </tr>
    <tr>
      <td style="text-align: center">Speeding up ImageNet Training on Supercomputers</td>
      <td style="text-align: center">SysML 2018</td>
      <td style="text-align: center">2018</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://arxiv.org/abs/1604.00981">Revisiting distributed synchronous SGD</a></td>
      <td style="text-align: center">ICLR 2016</td>
      <td style="text-align: center">2016-04</td>
    </tr>
  </tbody>
</table>

<h3 id="算法分析与优化">算法分析与优化</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">论文名称</th>
      <th style="text-align: center">发表刊物</th>
      <th style="text-align: center">发表时间</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1708.03888">Large Batch Training of Convolutional Networks</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2017-08</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1705.08741">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</a></td>
      <td style="text-align: center">NIPS 2017</td>
      <td style="text-align: center">2017-05</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1609.04836">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a></td>
      <td style="text-align: center">ICLR 2017</td>
      <td style="text-align: center">2016-09</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1904.00962">Reducing BERT Pre-Training Time from 3 Days to 76 Minutes</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2019-04</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1711.00489">Don’t Decay the Learning Rate, Increase the Batch Size</a></td>
      <td style="text-align: center">ICLR 2018</td>
      <td style="text-align: center">2017-11</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1810.01021">Large Batch Size Training of Neural Networks with Adversarial Training and Second-Order Information</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-10</td>
    </tr>
    <tr>
      <td style="text-align: center">Augment your batch: better training with larger batches</td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2019-01</td>
    </tr>
  </tbody>
</table>

<h2 id="通信拓扑">通信拓扑</h2>
<h3 id="ring-allreduce">Ring AllReduce</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">论文名称</th>
      <th style="text-align: center">发表刊物</th>
      <th style="text-align: center">发表时间</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1802.05799">Horovod: fast and easy distributed deep learning in TensorFlow</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-02</td>
    </tr>
  </tbody>
</table>

<h3 id="parameter-server">Parameter Server</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">论文名称</th>
      <th style="text-align: center">发表刊物</th>
      <th style="text-align: center">发表时间</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1801.09805">Parameter Box: High Performance Parameter Servers for Efficient Distributed Deep Neural Network Training</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-01</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://papers.nips.cc/paper/7678-bml-a-high-performance-low-cost-gradient-synchronization-algorithm-for-dml-training.pdf">BML: A High-performance, Low-cost Gradient Synchronization Algorithm for DML Training</a></td>
      <td style="text-align: center">NIPS 2018</td>
      <td style="text-align: center">2018</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://doi.acm.org/10.1145/2901318.2901323">GeePS: Scalable deep learning on distributed GPUs with a GPU-specialized parameter server</a></td>
      <td style="text-align: center">EuroSys 2016</td>
      <td style="text-align: center">2016-04</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf">Large Scale Distributed Deep Networks</a></td>
      <td style="text-align: center">NIPS 2012</td>
      <td style="text-align: center">2012</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1905.03960">Priority-based Parameter Propagation for Distributed DNN Training</a></td>
      <td style="text-align: center">SysML 2019</td>
      <td style="text-align: center">2019-05</td>
    </tr>
  </tbody>
</table>

<h3 id="其他">其他</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">论文名称</th>
      <th style="text-align: center">发表刊物</th>
      <th style="text-align: center">发表时间</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1808.02621">Parallax: Sparsity-aware Data Parallel Training of Deep Neural Networks</a></td>
      <td style="text-align: center">EuroSys 2019</td>
      <td style="text-align: center">2018-08</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://link.springer.com/10.1007/978-3-540-24685-5_1">Optimization of Collective Reduction Operations</a></td>
      <td style="text-align: center">ICCS 2004</td>
      <td style="text-align: center">2004-06</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1801.03855">MXNET-MPI: Embedding MPI parallelism in Parameter Server Task Model for scaling Deep Learning</a></td>
      <td style="text-align: center">Proceedings of ACM Conference</td>
      <td style="text-align: center">2018-01</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1804.05839">BigDL: A Distributed Deep Learning Framework for Big Data</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-04</td>
    </tr>
    <tr>
      <td style="text-align: center">Project Adam: Building an Efficient and Scalable Deep Learning Training System</td>
      <td style="text-align: center">OSDI 2014</td>
      <td style="text-align: center">2014</td>
    </tr>
    <tr>
      <td style="text-align: center">RedSync: Reducing synchronization bandwidth for distributed deep learning training system</td>
      <td style="text-align: center">JPDC 2019</td>
      <td style="text-align: center">2018</td>
    </tr>
  </tbody>
</table>

<h2 id="并行模式">并行模式</h2>
<h3 id="数据并行">数据并行</h3>
<h3 id="模型并行">模型并行</h3>
<h3 id="流水线训练">流水线训练</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">论文名称</th>
      <th style="text-align: center">发表刊物</th>
      <th style="text-align: center">发表时间</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1806.03377">PipeDream: Fast and Efficient Pipeline Parallel DNN Training</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-06</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1811.06965">GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-11</td>
    </tr>
  </tbody>
</table>

<h3 id="混合并行">混合并行</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">论文名称</th>
      <th style="text-align: center">发表刊物</th>
      <th style="text-align: center">发表时间</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1404.5997">One weird trick for parallelizing convolutional neural networks</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2014-04</td>
    </tr>
    <tr>
      <td style="text-align: center">Beyond Data and Model Parallelism for Deep Neural Networks</td>
      <td style="text-align: center">SysML 2019</td>
      <td style="text-align: center">2019</td>
    </tr>
  </tbody>
</table>

<h2 id="通信量化压缩">通信量化&amp;压缩</h2>

<p>|论文名称|发表刊物|发表时间|
| :——–: | :——–:| :——: |
|AdaComp: Adaptive Residual Gradient Compression for Data-Parallel Distributed Training|AAAI 2018|2018|
|<a href="http://ieeexplore.ieee.org/document/7835789/">Communication Quantization for Data-Parallel Training of Deep Neural Networks</a>|MLHPC|2016-11|
|<a href="http://ieeexplore.ieee.org/document/7835789/">Communication Quantization for Data-Parallel Training of Deep Neural Networks</a>|MLHPC|2016-11|
|<a href="http://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning.pdf">TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning</a>|NIPS 2017|2017|
|RedSync: Reducing synchronization bandwidth for distributed deep learning training system|JPDC 2019|2018|</p>
<h2 id="优化算法">优化算法</h2>
<h3 id="异步算法">异步算法</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">论文名称</th>
      <th style="text-align: center">发表刊物</th>
      <th style="text-align: center">发表时间</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1803.01113">Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD</a></td>
      <td style="text-align: center">AISTATS 2018</td>
      <td style="text-align: center">2018-03</td>
    </tr>
  </tbody>
</table>

<h3 id="模型平均">模型平均</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">论文名称</th>
      <th style="text-align: center">发表刊物</th>
      <th style="text-align: center">发表时间</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://arxiv.org/abs/1808.07217">Don’t Use Large Mini-Batches, Use Local SGD</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-08</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://dl.acm.org/citation.cfm?doid=3339186.3339203">Reducing global reductions in large-scale distributed training</a></td>
      <td style="text-align: center">ICPP 2019</td>
      <td style="text-align: center">2019-08</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1708.01012">On the convergence properties of a $K$-step averaging stochastic gradient descent algorithm for nonconvex optimization</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2017-08</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://ocs.aaai.org/ojs/index.php/AAAI/article/view/4465">Scalable Distributed DL Training: Batching Communication and Computation</a></td>
      <td style="text-align: center">AAAI 2019</td>
      <td style="text-align: center">2019</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1805.09767">Local SGD Converges Fast and Communicates Little</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-05</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1807.06629">Parallel Restarted SGD with Faster Convergence and Less Communication: Demystifying Why Model Averaging Works for Deep Learning</a></td>
      <td style="text-align: center">AAAI 2019</td>
      <td style="text-align: center">2018-07</td>
    </tr>
    <tr>
      <td style="text-align: center">A<a href="http://arxiv.org/abs/1810.08313">daptive Communication Strategies to Achieve the Best Error-Runtime Trade-off in Local-Update SGD</a></td>
      <td style="text-align: center">SysML 2019</td>
      <td style="text-align: center">2018-10</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://papers.nips.cc/paper/7117-can-decentralized-algorithms-outperform-centralized-algorithms-a-case-study-for-decentralized-parallel-stochastic-gradient-descent.pdf">Can Decentralized Algorithms OutperformCentralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent</a></td>
      <td style="text-align: center">NIPS 2017</td>
      <td style="text-align: center">2017</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1507.01239">Experiments on Parallel Training of Deep Neural Network using Model Averaging</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2015-07</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1808.07576">Cooperative SGD: A Unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-08</td>
    </tr>
  </tbody>
</table>

<h2 id="其他优化">其他优化</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: center">论文名称</th>
      <th style="text-align: center">发表刊物</th>
      <th style="text-align: center">发表时间</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1806.02508">Fast Distributed Deep Learning via Worker-adaptive Batch Sizing</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-06</td>
    </tr>
    <tr>
      <td style="text-align: center">Dynamic Mini-batch SGD for Elastic Distributed Training: Learning in the Limbo of Resources</td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2019-04</td>
    </tr>
  </tbody>
</table>

<h2 id="经典文章">经典文章</h2>
<h3 id="网络模型">网络模型</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">论文名称</th>
      <th style="text-align: center">发表刊物</th>
      <th style="text-align: center">发表时间</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></td>
      <td style="text-align: center">CVPR 2016</td>
      <td style="text-align: center">2015-12</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1602.02410">Exploring the Limits of Language Modeling</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2016-02</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://dl.acm.org/citation.cfm?doid=3098997.3065386">ImageNet classification with deep convolutional neural networks</a></td>
      <td style="text-align: center">NIPS 2012</td>
      <td style="text-align: center">2012</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1602.05629">Communication-Efficient Learning of Deep Networks from Decentralized Data</a></td>
      <td style="text-align: center">AISTATS 2017</td>
      <td style="text-align: center">2016-02</td>
    </tr>
  </tbody>
</table>

<h3 id="数据集">数据集</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">论文名称</th>
      <th style="text-align: center">发表刊物</th>
      <th style="text-align: center">发表时间</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1312.3005">One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2013-12</td>
    </tr>
  </tbody>
</table>
:ET