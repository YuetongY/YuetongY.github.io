I"—O<h1 id="æ–‡ç« æ±‡æ€»">æ–‡ç« æ±‡æ€»</h1>
<p>åœ¨è¿™é‡Œï¼Œæˆ‘å°†è¿‘æœŸè¯»è¿‡çš„ä¸€äº›æ–‡ç« åšä¸€ä¸ªæ±‡æ€»ä¸æ¢³ç†ã€‚ä¹‹åè¿™ç¯‡æ±‡æ€»æ¶‰åŠåˆ°çš„ç¯‡ç›®å°†éšç€é˜…è¯»çš„æ·±å…¥ä¸æ–­çš„å¢åŠ ã€‚
è¿™é‡ŒåŒ…æ‹¬çš„æ–‡ç« å¤§å¤šæ˜¯ä¸åˆ†å¸ƒå¼æ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒç›¸å…³çš„ï¼Œä¸è¿‡è¿™ä¸ªé¢†åŸŸåŒ…æ‹¬è®¸å¤šç»†åˆ†çš„æ–¹å‘ï¼Œå„ç¯‡æ–‡ç« ä¾§é‡çš„é‡ç‚¹æœ‰æ‰€ä¸åŒï¼Œå½“ç„¶ä¸åŒæ–¹å‘çš„æ–‡ç« çš„å…³æ³¨ç‚¹ä¹Ÿæœ‰äº¤é›†ã€‚ä¸ºäº†æ›´å¥½çš„æ•´ç†è¿™ä¸ªç›®å½•æ€§è´¨çš„æ–‡ç« æ±‡æ€»ï¼Œæˆ‘ä¾æ®ä¸ªäººç†è§£ç²—ç•¥å¯¹æ–‡ç« è¿›è¡Œåˆ†ç±»ï¼Œå¹¶æ”¾å…¥ç›¸åº”çš„ç±»åˆ«ä¸‹ã€‚æŸäº›æ–‡ç« å¦‚æœä¸å¤šä¸ªä¸»é¢˜ç›¸å…³ï¼Œå®ƒä»¬å°†ä¼šåœ¨å¤šä¸ªç±»åˆ«ä¸‹å‡ºç°ã€‚</p>

<p>[TOC]</p>

<h2 id="ç»¼è¿°">ç»¼è¿°</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: center">è®ºæ–‡åç§°</th>
      <th style="text-align: center">å‘è¡¨åˆŠç‰©</th>
      <th style="text-align: center">å‘è¡¨æ—¶é—´</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1802.09941">Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-02</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1810.11787">A Hitchhikerâ€™s Guide On Distributed Training of Deep Neural Networks</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-10</td>
    </tr>
  </tbody>
</table>

<h2 id="large-batch-training">Large Batch Training</h2>
<h3 id="ç³»ç»Ÿæ•´ä½“è®¾è®¡">ç³»ç»Ÿæ•´ä½“è®¾è®¡</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">è®ºæ–‡åç§°</th>
      <th style="text-align: center">å‘è¡¨åˆŠç‰©</th>
      <th style="text-align: center">å‘è¡¨æ—¶é—´</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://arxiv.org/abs/1706.02677">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2017-06</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://dl.acm.org/citation.cfm?id=3225069">ImageNet Training in Minutes</a></td>
      <td style="text-align: center">ICPP 2018</td>
      <td style="text-align: center">2018-08</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://arxiv.org/abs/1711.04325">Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2017-11</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://arxiv.org/abs/1807.11205">Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-07</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1902.06855">Optimizing Network Performance for Distributed DNN Training on GPU Clusters: ImageNet/AlexNet Training in 1.5 Minutes</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2019-02</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1810.01993">Exascale Deep Learning for Climate Analytics</a></td>
      <td style="text-align: center">SC 19</td>
      <td style="text-align: center">2018-10</td>
    </tr>
    <tr>
      <td style="text-align: center">Speeding up ImageNet Training on Supercomputers</td>
      <td style="text-align: center">SysML 2018</td>
      <td style="text-align: center">2018</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://arxiv.org/abs/1604.00981">Revisiting distributed synchronous SGD</a></td>
      <td style="text-align: center">ICLR 2016</td>
      <td style="text-align: center">2016-04</td>
    </tr>
  </tbody>
</table>

<h3 id="ç®—æ³•åˆ†æä¸ä¼˜åŒ–">ç®—æ³•åˆ†æä¸ä¼˜åŒ–</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">è®ºæ–‡åç§°</th>
      <th style="text-align: center">å‘è¡¨åˆŠç‰©</th>
      <th style="text-align: center">å‘è¡¨æ—¶é—´</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1708.03888">Large Batch Training of Convolutional Networks</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2017-08</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1705.08741">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</a></td>
      <td style="text-align: center">NIPS 2017</td>
      <td style="text-align: center">2017-05</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1609.04836">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a></td>
      <td style="text-align: center">ICLR 2017</td>
      <td style="text-align: center">2016-09</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1904.00962">Reducing BERT Pre-Training Time from 3 Days to 76 Minutes</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2019-04</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1711.00489">Donâ€™t Decay the Learning Rate, Increase the Batch Size</a></td>
      <td style="text-align: center">ICLR 2018</td>
      <td style="text-align: center">2017-11</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1810.01021">Large Batch Size Training of Neural Networks with Adversarial Training and Second-Order Information</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-10</td>
    </tr>
    <tr>
      <td style="text-align: center">Augment your batch: better training with larger batches</td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2019-01</td>
    </tr>
  </tbody>
</table>

<h2 id="é€šä¿¡æ‹“æ‰‘">é€šä¿¡æ‹“æ‰‘</h2>
<h3 id="ring-allreduce">Ring AllReduce</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">è®ºæ–‡åç§°</th>
      <th style="text-align: center">å‘è¡¨åˆŠç‰©</th>
      <th style="text-align: center">å‘è¡¨æ—¶é—´</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1802.05799">Horovod: fast and easy distributed deep learning in TensorFlow</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-02</td>
    </tr>
  </tbody>
</table>

<h3 id="parameter-server">Parameter Server</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">è®ºæ–‡åç§°</th>
      <th style="text-align: center">å‘è¡¨åˆŠç‰©</th>
      <th style="text-align: center">å‘è¡¨æ—¶é—´</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1801.09805">Parameter Box: High Performance Parameter Servers for Efficient Distributed Deep Neural Network Training</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-01</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://papers.nips.cc/paper/7678-bml-a-high-performance-low-cost-gradient-synchronization-algorithm-for-dml-training.pdf">BML: A High-performance, Low-cost Gradient Synchronization Algorithm for DML Training</a></td>
      <td style="text-align: center">NIPS 2018</td>
      <td style="text-align: center">2018</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://doi.acm.org/10.1145/2901318.2901323">GeePS: Scalable deep learning on distributed GPUs with a GPU-specialized parameter server</a></td>
      <td style="text-align: center">EuroSys 2016</td>
      <td style="text-align: center">2016-04</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf">Large Scale Distributed Deep Networks</a></td>
      <td style="text-align: center">NIPS 2012</td>
      <td style="text-align: center">2012</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1905.03960">Priority-based Parameter Propagation for Distributed DNN Training</a></td>
      <td style="text-align: center">SysML 2019</td>
      <td style="text-align: center">2019-05</td>
    </tr>
  </tbody>
</table>

<h3 id="å…¶ä»–">å…¶ä»–</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">è®ºæ–‡åç§°</th>
      <th style="text-align: center">å‘è¡¨åˆŠç‰©</th>
      <th style="text-align: center">å‘è¡¨æ—¶é—´</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1808.02621">Parallax: Sparsity-aware Data Parallel Training of Deep Neural Networks</a></td>
      <td style="text-align: center">EuroSys 2019</td>
      <td style="text-align: center">2018-08</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://link.springer.com/10.1007/978-3-540-24685-5_1">Optimization of Collective Reduction Operations</a></td>
      <td style="text-align: center">ICCS 2004</td>
      <td style="text-align: center">2004-06</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1801.03855">MXNET-MPI: Embedding MPI parallelism in Parameter Server Task Model for scaling Deep Learning</a></td>
      <td style="text-align: center">Proceedings of ACM Conference</td>
      <td style="text-align: center">2018-01</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1804.05839">BigDL: A Distributed Deep Learning Framework for Big Data</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-04</td>
    </tr>
    <tr>
      <td style="text-align: center">Project Adam: Building an Efficient and Scalable Deep Learning Training System</td>
      <td style="text-align: center">OSDI 2014</td>
      <td style="text-align: center">2014</td>
    </tr>
  </tbody>
</table>

<h2 id="å¹¶è¡Œæ¨¡å¼">å¹¶è¡Œæ¨¡å¼</h2>
<h3 id="æ•°æ®å¹¶è¡Œ">æ•°æ®å¹¶è¡Œ</h3>
<h3 id="æ¨¡å‹å¹¶è¡Œ">æ¨¡å‹å¹¶è¡Œ</h3>
<h3 id="æµæ°´çº¿è®­ç»ƒ">æµæ°´çº¿è®­ç»ƒ</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">è®ºæ–‡åç§°</th>
      <th style="text-align: center">å‘è¡¨åˆŠç‰©</th>
      <th style="text-align: center">å‘è¡¨æ—¶é—´</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1806.03377">PipeDream: Fast and Efficient Pipeline Parallel DNN Training</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-06</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1811.06965">GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-11</td>
    </tr>
  </tbody>
</table>

<h3 id="æ··åˆå¹¶è¡Œ">æ··åˆå¹¶è¡Œ</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">è®ºæ–‡åç§°</th>
      <th style="text-align: center">å‘è¡¨åˆŠç‰©</th>
      <th style="text-align: center">å‘è¡¨æ—¶é—´</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1404.5997">One weird trick for parallelizing convolutional neural networks</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2014-04</td>
    </tr>
    <tr>
      <td style="text-align: center">Beyond Data and Model Parallelism for Deep Neural Networks</td>
      <td style="text-align: center">SysML 2019</td>
      <td style="text-align: center">2019</td>
    </tr>
  </tbody>
</table>

<h2 id="é€šä¿¡é‡åŒ–å‹ç¼©">é€šä¿¡é‡åŒ–&amp;å‹ç¼©</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: center">è®ºæ–‡åç§°</th>
      <th style="text-align: center">å‘è¡¨åˆŠç‰©</th>
      <th style="text-align: center">å‘è¡¨æ—¶é—´</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">AdaComp: Adaptive Residual Gradient Compression for Data-Parallel Distributed Training</td>
      <td style="text-align: center">AAAI 2018</td>
      <td style="text-align: center">2018</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://ieeexplore.ieee.org/document/7835789/">Communication Quantization for Data-Parallel Training of Deep Neural Networks</a></td>
      <td style="text-align: center">MLHPC</td>
      <td style="text-align: center">2016-11</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://ieeexplore.ieee.org/document/7835789/">Communication Quantization for Data-Parallel Training of Deep Neural Networks</a></td>
      <td style="text-align: center">MLHPC</td>
      <td style="text-align: center">2016-11</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning.pdf">TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning</a></td>
      <td style="text-align: center">NIPS 2017</td>
      <td style="text-align: center">2017</td>
    </tr>
  </tbody>
</table>

<h2 id="ä¼˜åŒ–ç®—æ³•">ä¼˜åŒ–ç®—æ³•</h2>
<h3 id="å¼‚æ­¥ç®—æ³•">å¼‚æ­¥ç®—æ³•</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">è®ºæ–‡åç§°</th>
      <th style="text-align: center">å‘è¡¨åˆŠç‰©</th>
      <th style="text-align: center">å‘è¡¨æ—¶é—´</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1803.01113">Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD</a></td>
      <td style="text-align: center">AISTATS 2018</td>
      <td style="text-align: center">2018-03</td>
    </tr>
  </tbody>
</table>

<h3 id="æ¨¡å‹å¹³å‡">æ¨¡å‹å¹³å‡</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">è®ºæ–‡åç§°</th>
      <th style="text-align: center">å‘è¡¨åˆŠç‰©</th>
      <th style="text-align: center">å‘è¡¨æ—¶é—´</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://arxiv.org/abs/1808.07217">Donâ€™t Use Large Mini-Batches, Use Local SGD</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-08</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://dl.acm.org/citation.cfm?doid=3339186.3339203">Reducing global reductions in large-scale distributed training</a></td>
      <td style="text-align: center">ICPP 2019</td>
      <td style="text-align: center">2019-08</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1708.01012">On the convergence properties of a $K$-step averaging stochastic gradient descent algorithm for nonconvex optimization</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2017-08</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://ocs.aaai.org/ojs/index.php/AAAI/article/view/4465">Scalable Distributed DL Training: Batching Communication and Computation</a></td>
      <td style="text-align: center">AAAI 2019</td>
      <td style="text-align: center">2019</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1805.09767">Local SGD Converges Fast and Communicates Little</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-05</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1807.06629">Parallel Restarted SGD with Faster Convergence and Less Communication: Demystifying Why Model Averaging Works for Deep Learning</a></td>
      <td style="text-align: center">AAAI 2019</td>
      <td style="text-align: center">2018-07</td>
    </tr>
    <tr>
      <td style="text-align: center">A<a href="http://arxiv.org/abs/1810.08313">daptive Communication Strategies to Achieve the Best Error-Runtime Trade-off in Local-Update SGD</a></td>
      <td style="text-align: center">SysML 2019</td>
      <td style="text-align: center">2018-10</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://papers.nips.cc/paper/7117-can-decentralized-algorithms-outperform-centralized-algorithms-a-case-study-for-decentralized-parallel-stochastic-gradient-descent.pdf">Can Decentralized Algorithms OutperformCentralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent</a></td>
      <td style="text-align: center">NIPS 2017</td>
      <td style="text-align: center">2017</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1507.01239">Experiments on Parallel Training of Deep Neural Network using Model Averaging</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2015-07</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1808.07576">Cooperative SGD: A Unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-08</td>
    </tr>
  </tbody>
</table>

<h2 id="å…¶ä»–ä¼˜åŒ–">å…¶ä»–ä¼˜åŒ–</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: center">è®ºæ–‡åç§°</th>
      <th style="text-align: center">å‘è¡¨åˆŠç‰©</th>
      <th style="text-align: center">å‘è¡¨æ—¶é—´</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1806.02508">Fast Distributed Deep Learning via Worker-adaptive Batch Sizing</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2018-06</td>
    </tr>
    <tr>
      <td style="text-align: center">Dynamic Mini-batch SGD for Elastic Distributed Training: Learning in the Limbo of Resources</td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2019-04</td>
    </tr>
  </tbody>
</table>

<h2 id="ç»å…¸æ–‡ç« ">ç»å…¸æ–‡ç« </h2>
<h3 id="ç½‘ç»œæ¨¡å‹">ç½‘ç»œæ¨¡å‹</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">è®ºæ–‡åç§°</th>
      <th style="text-align: center">å‘è¡¨åˆŠç‰©</th>
      <th style="text-align: center">å‘è¡¨æ—¶é—´</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></td>
      <td style="text-align: center">CVPR 2016</td>
      <td style="text-align: center">2015-12</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1602.02410">Exploring the Limits of Language Modeling</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2016-02</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://dl.acm.org/citation.cfm?doid=3098997.3065386">ImageNet classification with deep convolutional neural networks</a></td>
      <td style="text-align: center">NIPS 2012</td>
      <td style="text-align: center">2012</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1602.05629">Communication-Efficient Learning of Deep Networks from Decentralized Data</a></td>
      <td style="text-align: center">AISTATS 2017</td>
      <td style="text-align: center">2016-02</td>
    </tr>
  </tbody>
</table>

<h3 id="æ•°æ®é›†">æ•°æ®é›†</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">è®ºæ–‡åç§°</th>
      <th style="text-align: center">å‘è¡¨åˆŠç‰©</th>
      <th style="text-align: center">å‘è¡¨æ—¶é—´</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="http://arxiv.org/abs/1312.3005">One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling</a></td>
      <td style="text-align: center">arXiv</td>
      <td style="text-align: center">2013-12</td>
    </tr>
  </tbody>
</table>
:ET