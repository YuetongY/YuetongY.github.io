I"4´<h3 id="èƒŒæ™¯">èƒŒæ™¯</h3>
<p>åœ¨CNNç­‰æ·±åº¦ç¥ç»ç½‘ç»œçš„åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œæ¯”è¾ƒå¸¸ç”¨çš„ä¸€ç§è®­ç»ƒæ¨¡å¼æ˜¯ï¼ˆåŒæ­¥çš„ï¼‰æ•°æ®å¹¶è¡Œï¼Œä¹Ÿå°±æ˜¯å„ä¸ªè®¡ç®—è®¾å¤‡åˆ†åˆ«æ ¹æ®å„è‡ªè·å¾—çš„batchï¼Œå‰å‘è®¡ç®—è·å¾—æŸå¤±ï¼Œè¿›è€Œåå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ã€‚å¾…æ‰€æœ‰è®¡ç®—è®¾å¤‡å®Œæˆæ¢¯åº¦è®¡ç®—ä¹‹åï¼Œå¯¹æ¢¯åº¦è¿›è¡Œå¹³å‡ï¼Œåˆ©ç”¨å¹³å‡æ¢¯åº¦å¯¹æ¨¡å‹è¿›è¡Œæ›´æ–°ã€‚</p>

<p>ä¸Šé¢è¿™ç§æ¨¡å¼åªæ˜¯ä¸€ç§é€»è¾‘æ¶æ„ï¼Œå…·ä½“å®ç°ä¸Šï¼Œå¯ä»¥ä½¿ç”¨å‚æ•°æœåŠ¡å™¨ï¼ˆPSï¼‰çš„å½¢å¼å®ç°ã€‚åœ¨å‚æ•°æœåŠ¡å™¨æ¶æ„ä¸­ï¼Œè®¡ç®—è®¾å¤‡è¢«åˆ’åˆ†ä¸ºå‚æ•°æœåŠ¡å™¨(ps)å’Œworkerã€‚å¯¹äºpsï¼Œé¡¾åæ€ä¹‰ï¼Œä¸»è¦æ˜¯å¯¹æ¨¡å‹çš„å‚æ•°è¿›è¡Œå­˜å‚¨ï¼›è€Œå¯¹äºworkerï¼Œä¸»è¦çš„å·¥ä½œæ˜¯å®Œæˆå‰å‘å’Œåå‘ä¼ æ’­è¿™ç±»è®¡ç®—å¯†é›†çš„è¿ç®—ã€‚</p>

<p>åœ¨ï¼ˆåŒæ­¥çš„ï¼‰å‚æ•°æœåŠ¡å™¨æ¶æ„ä¸­ï¼Œæ•´ä¸ªæµç¨‹å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š</p>
<ol>
  <li>workerä»psæŠŠæ¨¡å‹å‚æ•°pullåˆ°æœ¬åœ°çš„memoryä¸­</li>
  <li>workeråˆ©ç”¨æ¨¡å‹å‚æ•°å®Œæˆå‰å‘è®¡ç®—ï¼Œå†å®Œæˆåå‘ä¼ æ’­ï¼Œå¾—åˆ°æ¨¡å‹å‚æ•°çš„æ¢¯åº¦</li>
  <li>workerå°†æ¨¡å‹å‚æ•°çš„æ¢¯åº¦pushè‡³å‚æ•°æœåŠ¡å™¨ï¼Œå‚æ•°æœåŠ¡å™¨æ”¶åˆ°æ‰€æœ‰workerçš„æ¢¯åº¦ä¹‹åï¼Œè®¡ç®—å¹³å‡æ¢¯åº¦ï¼Œå¹¶å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œæ›´æ–°</li>
</ol>

<p>ä¹‹æ‰€ä»¥æƒ³åˆ°æŸ¥çœ‹<code class="highlighter-rouge">SyncReplicasOptimizer</code>çš„å®ç°ï¼Œæ˜¯å› ä¸ºæˆ‘åœ¨å®éªŒä¸­ï¼Œç”¨åˆ°äº†è¿™ä¸ªOptimizerï¼Œæœ€åˆçœ‹å®ƒçš„åå­—ï¼Œæƒ³å½“ç„¶çš„è®¤ä¸ºå®ƒæ˜¯ä¸€ä¸ªåŒæ­¥çš„Optimizerï¼Œä½†æ˜¯å®é™…ä½¿ç”¨å‘ç°å¦‚æœå‚æ•°è®¾ç½®ä¸å½“ï¼Œå¯èƒ½ä¼šå‡ºç°å„ä¸ªworkerè®¡ç®—è¿›åº¦ä¸ä¸€è‡´ï¼Œä¹Ÿå°±æ˜¯æ²¡æœ‰çœŸæ­£çš„åŒæ­¥ã€‚å¦å¤–ï¼Œåœ¨å®éªŒä¸­å‘ç°åœ¨æˆ‘çš„å®éªŒç¯å¢ƒä¸‹ï¼ŒPSæ¶æ„ + <code class="highlighter-rouge">sync_replicas_optimizer</code>ä¸èƒ½å®Œå…¨å‘æŒ¥é›†ç¾¤çš„æ€§èƒ½ï¼Œå¯èƒ½éœ€è¦äº†è§£sync_replicas_optimizerçš„å·¥ä½œåŸç†æ‰èƒ½æ›´å¥½çš„å‘ç°é—®é¢˜æ‰€åœ¨ã€‚</p>

<p>ä¸‹é¢çš„ä»‹ç»æ˜¯æˆ‘ä¸ªäººæŸ¥çœ‹<code class="highlighter-rouge">tf.train.Sync_replicas_optimizer</code>å®ç°éƒ¨åˆ†çš„ç†è§£ã€‚</p>

<h3 id="syncreplicasoptimizerè§£æ">SyncReplicasOptimizerè§£æ</h3>
<h4 id="æ–‡æ¡£è¯´æ˜">æ–‡æ¡£è¯´æ˜</h4>
<p>åœ¨TensorFlow r1.14ç‰ˆæœ¬çš„<a href="https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer">æ–‡æ¡£</a>ä¸­ï¼Œè¿™ä¸ªOptimizeræä¾›çš„APIå·²ç»è¢«å¼ƒç”¨äº†ï¼Œåœ¨æ–°çš„APIä¸­ï¼Œå¦‚æœå¸Œæœ›åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­å®ç°åŒæ­¥çš„è®­ç»ƒï¼Œæ˜¯é€šè¿‡é…ç½®<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute">Distribution Strategies</a>æ¥å®ç°çš„ã€‚</p>
<blockquote>
  <p>This class is deprecated. For synchrononous training, please use <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute">Distribution Strategies</a>.</p>
</blockquote>

<p>åœ¨æ–‡æ¡£ä¸­ï¼Œç»™å‡ºäº†è¿™ä¸ªä¼˜åŒ–å™¨çš„â€œåŒæ­¥â€è®­ç»ƒæ–¹æ¡ˆï¼Œå¦‚ä¸‹ï¼š</p>
<blockquote>
  <ul>
    <li>For the Parameter Server job:
      <ol>
        <li>An accumulator is created for each variable, and each replica pushes the gradients into the accumulators instead of directly applying them to the variables.</li>
        <li>Each accumulator averages once enough gradients (replicas_to_aggregate) have been accumulated.</li>
        <li>Apply the averaged gradients to the variables.</li>
        <li>Only after all variables have been updated, increment the global step.</li>
        <li>Only after step 4, pushes global_step in the token_queue, once for each worker replica. The workers can now fetch the global step, use it to update its local_step variable and start the next batch. Please note that some workers can consume multiple minibatches, while some may not consume even one. This is because each worker fetches minibatches as long as a token exists. If one worker is stuck for some reason and does not consume a token, another worker can use it.</li>
      </ol>
    </li>
    <li>For the replicas:
      <blockquote>
        <ol>
          <li>Start a step: fetch variables and compute gradients.</li>
          <li>Once the gradients have been computed, push them into gradient accumulators. Each accumulator will check the staleness and drop the stale.</li>
          <li>After pushing all the gradients, dequeue an updated value of global_step from the token queue and record that step to its local_step variable. Note that this is effectively a barrier.</li>
          <li>Start the next batch.</li>
        </ol>
      </blockquote>
    </li>
  </ul>
</blockquote>

<p>åœ¨æ–‡æ¡£ä¸­ï¼ŒParameter Server jobå¯ä»¥ç®€å•ç†è§£ä¸ºå‚æ•°æœåŠ¡å™¨ï¼Œè€ŒReplicaså¯ä»¥ç®€å•ç†è§£ä¸ºå„ä¸ªworkerï¼Œæˆ–è€…æ˜¯å„ä¸ªGPUã€‚å¯ä»¥ä»æ–‡æ¡£ä¸­å¾—åˆ°è¿™ä¸¤ä¸ªä¿¡æ¯ï¼š</p>
<ul>
  <li>å‚æ•°æœåŠ¡å™¨æ˜¯é€šè¿‡ä¸ºæ¯ä¸€ä¸ªvariableå»ºç«‹ä¸€ä¸ª<code class="highlighter-rouge">accumulator</code>æ•°æ®ç»“æ„ï¼Œè¿›è€Œå¯¹æ¥è‡ªä¸åŒworkerçš„æ¢¯åº¦è¿›è¡Œç®¡ç†ï¼ˆå¹³å‡ï¼‰çš„</li>
  <li><code class="highlighter-rouge">token_queue</code>è¿™ä¸ªæ•°æ®ç»“æ„è¢«ç”¨æ¥ä¿æŒå„ä¸ªworkerçš„è¿›åº¦ï¼ˆä¸€è‡´ï¼‰</li>
</ul>

<p>æ–‡æ¡£ä¸­çš„å™è¿°å±•ç¤ºäº†è¿™ä¸ªOptimizerçš„æ‰§è¡Œé€»è¾‘ï¼Œä½†æ˜¯å¯¹äºTensorFlowè€Œè¨€ï¼Œå®ƒæ˜¯é€šè¿‡è®¡ç®—å›¾å®Œæˆè®¡ç®—çš„ï¼Œä¸Šé¢çš„æµç¨‹å¦‚ä½•åœ¨è®¡ç®—å›¾ä¸­å¾—åˆ°ä½“ç°ï¼Œå°±éœ€è¦æŸ¥çœ‹<code class="highlighter-rouge">sync_replicas_optimizer</code>çš„å®ç°éƒ¨åˆ†ã€‚</p>
<h4 id="æºç éƒ¨åˆ†">æºç éƒ¨åˆ†</h4>
<h5 id="__init__"><code class="highlighter-rouge">__init__</code></h5>
<p><code class="highlighter-rouge">sync_replicas_optimizer</code>æ˜¯ä¸€ä¸ªwrapper optimizerï¼Œä¹Ÿå°±æ˜¯è¿™ä¸ªOptimizerå¯¹å…¶ä»–Optimizerè¿›è¡ŒåŒ…è£…ï¼Œå®Œæˆworkeré—´æ¢¯åº¦åŒæ­¥çš„å·¥ä½œï¼Œè€Œå®é™…æ¢¯åº¦çš„è®¡ç®—åˆ™æ˜¯äº¤ç”±è¢«åŒ…è£…çš„Optimizeræ¥å®Œæˆçš„ã€‚è¿™ä¸€ç‚¹å¯ä»¥é€šè¿‡æ„é€ å‡½æ•°å‘ç°ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">opt</span><span class="p">,</span>
               <span class="n">replicas_to_aggregate</span><span class="p">,</span>
               <span class="n">total_num_replicas</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">variable_averages</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">variables_to_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">use_locking</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s">"sync_replicas"</span><span class="p">):</span>
               <span class="k">if</span> <span class="n">total_num_replicas</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
				    <span class="n">total_num_replicas</span> <span class="o">=</span> <span class="n">replicas_to_aggregate</span>
			    <span class="nb">super</span><span class="p">(</span><span class="n">SyncReplicasOptimizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">use_locking</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
			    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
			        <span class="s">"SyncReplicasV2: replicas_to_aggregate=</span><span class="si">%</span><span class="s">s; total_num_replicas=</span><span class="si">%</span><span class="s">s"</span><span class="p">,</span>
			        <span class="n">replicas_to_aggregate</span><span class="p">,</span> <span class="n">total_num_replicas</span><span class="p">)</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_opt</span> <span class="o">=</span> <span class="n">opt</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_replicas_to_aggregate</span> <span class="o">=</span> <span class="n">replicas_to_aggregate</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_gradients_applied</span> <span class="o">=</span> <span class="bp">False</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_variable_averages</span> <span class="o">=</span> <span class="n">variable_averages</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_variables_to_average</span> <span class="o">=</span> <span class="n">variables_to_average</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_total_num_replicas</span> <span class="o">=</span> <span class="n">total_num_replicas</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_tokens_per_step</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">total_num_replicas</span><span class="p">,</span> <span class="n">replicas_to_aggregate</span><span class="p">)</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span> <span class="o">=</span> <span class="bp">None</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_sync_token_queue</span> <span class="o">=</span> <span class="bp">None</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_chief_queue_runner</span> <span class="o">=</span> <span class="bp">None</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_accumulator_list</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></div>

<p>åœ¨æ„é€ å‡½æ•°ä¸­ï¼Œå¯ä»¥ä¼ å…¥è¿™ä¹ˆå‡ ä¸ªå‚æ•°ï¼Œ<code class="highlighter-rouge">opt</code>,<code class="highlighter-rouge">replicas_to_aggregate</code>,<code class="highlighter-rouge">total_num_replicas</code>,<code class="highlighter-rouge">variable_averages</code>,<code class="highlighter-rouge">variables_to_average</code>,<code class="highlighter-rouge">use_locking</code>,<code class="highlighter-rouge">name</code>ã€‚å„ä¸ªå‚æ•°å…·ä½“çš„å«ä¹‰å¯ä»¥å‚è€ƒ<a href="https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer#__init__">å®˜æ–¹æ–‡æ¡£</a>ç»™å‡ºçš„è§£é‡Šã€‚éœ€è¦è¯´æ˜çš„æœ‰å‡ ä¸ªç‚¹ï¼Œ<code class="highlighter-rouge">opt</code>è¿™ä¸ªå‚æ•°ä½“ç°äº†wrapperçš„æ€æƒ³ï¼Œå®ƒæ˜¯ä¸€ä¸ª<code class="highlighter-rouge">Optimizer</code>ç±»çš„ä¸€ä¸ªå¯¹è±¡ï¼Œé€šè¿‡å®ƒæ¥çœŸæ­£å®Œæˆæ¢¯åº¦çš„è®¡ç®—ã€‚<code class="highlighter-rouge">total_num_replicas</code>å’Œ<code class="highlighter-rouge">replicas_to_aggregate</code>è¿™ä¸¤ä¸ªå‚æ•°ä¼šè¢«ç”¨æ¥æ§åˆ¶åŒæ­¥çš„è¿›åº¦ï¼Œå‰è€…è¡¨ç¤º<strong>é›†ç¾¤ä¸­workerçš„æ•°ç›®</strong>ï¼Œè€Œåè€…è¡¨ç¤º<strong>pså®Œæˆä¸€æ¬¡å‚æ•°æ›´æ–°æ‰€éœ€è¦æ”¶é›†çš„æ¢¯åº¦æ•°ç›®</strong>ã€‚åœ¨åŒæ­¥çš„è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œå®ƒä»¬ä¸¤ä¸ªåº”å½“æ˜¯ç›¸ç­‰çš„ï¼Œä½†æ˜¯äº‹å®ä¸Šï¼Œå³ä½¿å®ƒä»¬ä¿©ç›¸ç­‰ï¼Œä¹Ÿæœ‰å¯èƒ½å¯¼è‡´ä¸åŒæ­¥çš„ç°è±¡å‡ºç°ï¼Œè¿™ä¸å…¶ä»–å‚æ•°(<code class="highlighter-rouge">token_num</code>)çš„è®¾ç½®æœ‰å…³ç³»ã€‚</p>

<p>å¦‚æœå‰è€…æ¯”åè€…å¤§ï¼Œé€»è¾‘ä¸Šæ„å‘³ç€psåœ¨ä¸€è½®è¿­ä»£ä¸­ï¼Œåªéœ€è¦æ”¶é›†åˆ°éƒ¨åˆ†æ¢¯åº¦ï¼Œå°±åˆ©ç”¨è¿™äº›æ¢¯åº¦è®¡ç®—å¹³å‡æ¢¯åº¦ï¼Œè¿›è€Œå¯¹å‚æ•°æ›´æ–°ï¼Œè¿™å¯èƒ½æ„å‘³ç€è¿™è½®è¿­ä»£ä¸­ï¼Œè®¡ç®—è¾ƒæ…¢çš„é‚£ä¸ªèŠ‚ç‚¹çš„æ¢¯åº¦ä¼šè¢«ä¸¢å¼ƒ<em>ï¼ˆåªæ˜¯ä¸€ç§çŒœæµ‹ï¼Œè¿˜æœªå®éªŒéªŒè¯ï¼‰</em>ã€‚è¿™ä¸¤ä¸ªå‚æ•°ä¸ä¹‹å‰æåˆ°çš„<code class="highlighter-rouge">token_queue</code>çš„è®¾ç½®æœ‰å…³ç³»ï¼Œåœ¨åé¢çš„æºç ä»‹ç»å°†ä¼šæ›´åŠ è¯¦ç»†çš„è¯´æ˜ã€‚</p>

<p>åœ¨æ„é€ å‡½æ•°ä¸­ï¼Œè¿˜ä¸ºè¿™ä¸ªOptimizerå®šä¹‰äº†ä¸€äº›æ–°çš„æ•°æ®ç»“æ„ï¼Œå°±ä¸è¯¦ç»†ä»‹ç»äº†ï¼Œè¯»å®Œæ•´ä¸ªæ‰§è¡Œæµç¨‹ä¹‹åï¼Œå®ƒä»¬çš„åŠŸèƒ½åº”è¯¥å°±ä¼šæ¯”è¾ƒæ¸…æ¥šã€‚</p>

<h5 id="compute_gradients"><code class="highlighter-rouge">compute_gradients</code></h5>
<p>ä¸‹é¢æ¥çœ‹<code class="highlighter-rouge">compute_gradients</code>çš„å®ç°ã€‚</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="k">def</span> <span class="nf">compute_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
	   <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_opt</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></div>

<p>å¯ä»¥ä»æºç ä¸­éªŒè¯wrapperçš„æ€æƒ³ï¼Œæ¢¯åº¦çš„è®¡ç®—æ˜¯é€šè¿‡è¢«åŒ…è£…çš„Optimizeræ¥å®ç°çš„ã€‚</p>

<h5 id="apply_gradients"><code class="highlighter-rouge">apply_gradients</code></h5>
<p>è¿™ä¸ªæ–¹æ³•å°†ä¼šæ„é€ æ¨¡å‹å‚æ•°æ›´æ–°éƒ¨åˆ†çš„è®¡ç®—å›¾ï¼Œæœ€åå°†è¿”å›ä¸€ä¸ª<code class="highlighter-rouge">train_op</code>ã€‚<code class="highlighter-rouge">train_op</code>æ˜¯é€šå¸¸è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œclientä¸ºsessionçš„fetchesæä¾›çš„å‚æ•°ä¹‹ä¸€ï¼Œä¹Ÿå°±æ˜¯è¿™ä¸ªOperationè¢«æ‰§è¡Œä¹‹åï¼Œæ¨¡å‹çš„å‚æ•°å°†ä¼šå®Œæˆæ›´æ–°ï¼Œå¹¶å¼€å§‹ä¸‹ä¸€ä¸ªbatchçš„è®­ç»ƒã€‚é‚£ä¹ˆè¿™ä¹Ÿå°±æ„å‘³ç€ï¼Œè¿™ä¸ªæ–¹æ³•ä¸­æ¶‰åŠåˆ°çš„è®¡ç®—å›¾å°†ä¼šå®ç°è¯´æ˜æ–‡æ¡£ä¸­çš„è®­ç»ƒé€»è¾‘ã€‚</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">apply_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads_and_vars</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">grads_and_vars</span><span class="p">:</span>
      <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Must supply at least one variable"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">global_step</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Global step is required to check staleness"</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span> <span class="o">=</span> <span class="n">global_step</span>
    <span class="n">train_ops</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># æ‰€æœ‰çš„train_opï¼Œä¹Ÿå°±æ˜¯Accumulatorçš„apply_gradientsï¼Œå®é™…ä¸Šå¯ä»¥æŒ‡pushçš„è¿‡ç¨‹
</span>    <span class="n">aggregated_grad</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># å¹³å‡åçš„æ¢¯åº¦
</span>    <span class="n">var_list</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># å˜é‡å­˜æ”¾çš„åˆ—è¡¨
</span>    <span class="o">...</span>
</code></pre></div></div>

<p>ä¸Šé¢çš„ä»£ç è´Ÿè´£éªŒè¯ä¼ å…¥æ–¹æ³•çš„å‚æ•°çš„åˆæ³•æ€§ï¼Œå¹¶åˆ›å»ºä¸€äº›ä¾›æ–¹æ³•å†…éƒ¨ä½¿ç”¨çš„æ•°æ®ç»“æ„ã€‚ä¼ å…¥çš„å‚æ•°å¿…é¡»åŒ…å«æœ‰<code class="highlighter-rouge">global_step</code>ï¼Œè¿™åœ¨å…¶ä»–Optimizerå¯èƒ½ä¸æ˜¯ä¸€ä¸ªå¿…é¡»çš„å‚æ•°ï¼Œåœ¨è¿™é‡Œï¼Œå®ƒä¸ºå¿…é¡»çš„ï¼Œå› ä¸ºå®ƒéœ€è¦è¢«ç”¨åœ¨æ£€æŸ¥stale gradietsçš„åœ°æ–¹ã€‚</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">local_anchor</span> <span class="o">=</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="n">local_anchor</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_local_step</span> <span class="o">=</span> <span class="n">variable_scope</span><span class="o">.</span><span class="n">variable</span><span class="p">(</span>
          <span class="n">initial_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
          <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">LOCAL_VARIABLES</span><span class="p">],</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">global_step</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="s">"sync_rep_local_step"</span><span class="p">)</span>
      <span class="c1"># _local_stepçš„åˆå§‹åŒ–Operation
</span>      <span class="bp">self</span><span class="o">.</span><span class="n">local_step_init_op</span> <span class="o">=</span> <span class="n">state_ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_local_step</span><span class="p">,</span> <span class="n">global_step</span><span class="p">)</span>
      <span class="n">chief_init_ops</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">local_step_init_op</span><span class="p">]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">ready_for_local_init_op</span> <span class="o">=</span> <span class="n">variables</span><span class="o">.</span><span class="n">report_uninitialized_variables</span><span class="p">(</span>
        <span class="n">variables</span><span class="o">.</span><span class="n">global_variables</span><span class="p">())</span>
</code></pre></div></div>

<p>è¿™ä¸€éƒ¨åˆ†é¦–å…ˆåœ¨è®¡ç®—å›¾ä¸Šåˆ›å»ºäº†ä¸€ä¸ªç©ºçš„Operationï¼Œä¹‹ååˆ›å»ºäº†ä¸€ä¸ªç±»çš„æˆå‘˜å˜é‡<code class="highlighter-rouge">_local_step</code>ï¼Œç”¨æ¥è®°å½•è¿™ä¸ªworkerçš„è®¡ç®—è¿›åº¦ã€‚ç”¨åˆ°<code class="highlighter-rouge">ops.colocate_with()</code>çš„åŸå› æ˜¯åœ¨äºï¼Œåœ¨ä¸€èˆ¬æƒ…å†µä¸‹ï¼ŒPSæ¶æ„ä¼šé€šè¿‡device functionç­‰æœºåˆ¶ï¼Œå°†è¿™ç±»Operationæ”¾åœ¨workeræ‰€å¤„çš„deviceä¸Šï¼Œè€Œå°†Variablesè¿™ç±»ç‰¹æ®Šçš„Operationæ”¾åœ¨PSä¸Šã€‚<code class="highlighter-rouge">local_anchor</code>æ˜¯ä¸€ä¸ªä¸€èˆ¬çš„Operationï¼Œå› æ­¤å®ƒä¼šè¢«PSæ¶æ„åˆ†é…è‡³workeræ‰€å¤„çš„deviceï¼Œè¿™æ ·å°±èƒ½ç¡®ä¿åˆ›å»ºçš„<code class="highlighter-rouge">_local_step</code>å˜é‡ä¹Ÿèƒ½å¤Ÿæ”¾åœ¨workerä¸Šã€‚</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">):</span>  <span class="c1"># åœ¨è®¡ç®—å›¾ä¸­åˆ›å»ºä»¥è¿™ä¸ªä¼˜åŒ–å™¨ä¸ºåç§°çš„name_scope
</span>      <span class="k">for</span> <span class="n">grad</span><span class="p">,</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">grads_and_vars</span><span class="p">:</span>  <span class="c1"># éå†è®¡ç®—å¥½çš„æ¯ä¸€ä»½æ¢¯åº¦
</span>        <span class="n">var_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>  <span class="c1"># å°†variablesæŒ‰é¡ºåºä¿å­˜åœ¨ä¸€ä¸ªåˆ—è¡¨é‡Œ
</span>        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>  <span class="c1"># ä¸‹é¢å®šä¹‰çš„è®¡ç®—å›¾çš„éƒ¨åˆ†å°†ä¸variablesæ”¾åœ¨ç›¸åŒçš„deviceä¸Šã€‚å¯ä»¥ç†è§£ä¸ºéƒ½è¢«æ”¾åœ¨PSä¸Š
</span>          <span class="c1"># Dense gradients.
</span>          <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">aggregated_grad</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span>  <span class="c1"># pass-through.
</span>            <span class="k">continue</span>
          <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="c1"># ä¸ºæ¯ä¸€ä»½æ¢¯åº¦åˆ›å»ºä¸€ä¸ªConditionalAccumulator
</span>            <span class="n">grad_accum</span> <span class="o">=</span> <span class="n">data_flow_ops</span><span class="o">.</span><span class="n">ConditionalAccumulator</span><span class="p">(</span>
                <span class="n">grad</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">get_shape</span><span class="p">(),</span>
                <span class="n">shared_name</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s">"/grad_accum"</span><span class="p">)</span>
            <span class="c1"># åœ¨train_opsè¿™ä¸ªé›†åˆä¸­æ”¾å…¥å¯¹åº”accumulatorçš„apply_grad operation
</span>            <span class="c1"># æ³¨æ„ï¼Œè¿™ä¸ªapply_grad Operationä¼ å…¥äº†ä¸€ä¸ªlocal_stepå‚æ•°ï¼Œè¿™ä¸ªæ“Operationæ˜¯æŠŠæ¢¯åº¦æ”¾åˆ°accumulatorä¸­ï¼Œå¦‚æœlocal_stepè½åäºglobal_stepï¼Œè¿™ä¸ªOperationä¼šè‡ªåŠ¨ç»ˆæ­¢ï¼Œä¹Ÿå°±æ˜¯æ¢¯åº¦ä¸ä¼šæ”¾å…¥accumulator
</span>            <span class="n">train_ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_accum</span><span class="o">.</span><span class="n">apply_grad</span><span class="p">(</span>
                <span class="n">grad</span><span class="p">,</span> <span class="n">local_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_local_step</span><span class="p">))</span>
            <span class="c1"># aggregrated_gradä¸­æœ‰æ‰€æœ‰å¹³å‡ä¹‹åçš„æ¢¯åº¦ï¼Œå®ƒæ˜¯ä»accumulatorä¸­æ‰§è¡Œtake_gradçš„Operation
</span>            <span class="c1"># take_grad()ä¸­æœ‰ä¸€ä¸ªå‚æ•°ï¼Œnum_requiredï¼Œå¦‚æœè¯´accumulatorä¸­çš„æ¢¯åº¦æ•°é‡å°‘äºnum_requiredï¼Œå®ƒå°†ä¼šé˜»å¡ã€‚
</span>            <span class="n">aggregated_grad</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_accum</span><span class="o">.</span><span class="n">take_grad</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_replicas_to_aggregate</span><span class="p">))</span>
          <span class="k">else</span><span class="p">:</span>
		    <span class="c1"># ç¨€ç–æ¢¯åº¦çš„å¤„ç†ï¼Œä¸ä¸Šé¢ç±»ä¼¼
</span>            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
              <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Unknown grad type!"</span><span class="p">)</span>
            <span class="n">grad_accum</span> <span class="o">=</span> <span class="n">data_flow_ops</span><span class="o">.</span><span class="n">SparseConditionalAccumulator</span><span class="p">(</span>
                <span class="n">grad</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">shared_name</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s">"/grad_accum"</span><span class="p">)</span>
            <span class="n">train_ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_accum</span><span class="o">.</span><span class="n">apply_indexed_slices_grad</span><span class="p">(</span>
                <span class="n">grad</span><span class="p">,</span> <span class="n">local_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_local_step</span><span class="p">))</span>
            <span class="n">aggregated_grad</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_accum</span><span class="o">.</span><span class="n">take_indexed_slices_grad</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_replicas_to_aggregate</span><span class="p">))</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">_accumulator_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">grad_accum</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
      <span class="n">aggregated_grads_and_vars</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">aggregated_grad</span><span class="p">,</span> <span class="n">var_list</span><span class="p">)</span>  <span class="c1"># æœ€ç»ˆçš„(å¹³å‡æ¢¯åº¦ï¼Œå‚æ•°)åˆ—è¡¨
</span></code></pre></div></div>

<p>ä¸Šé¢çš„ä»£ç ç»™å‡ºçš„æ˜¯è¿›è¡Œæ¢¯åº¦èšåˆï¼Œæœ€ç»ˆè·å¾—å¯¹åº”æ¯ä¸€ä¸ªå‚æ•°çš„å¹³å‡æ¢¯åº¦çš„è¿‡ç¨‹ã€‚åœ¨ä¸Šé¢è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œåˆ†åˆ«ä¸ºæ¯ä¸€ä¸ªVariableåˆ›å»ºäº†ä¸€ä¸ª<code class="highlighter-rouge">ConditionalAccumulator</code>ï¼Œ<code class="highlighter-rouge">ConditionalAccumulator</code>æ˜¯è¢«å¤šä¸ªSessionæ‰€å…±äº«çš„ï¼Œç”¨äºç»´æŠ¤åœ¨è¿™ä¸ªtime stepä¸‹æ¥è‡ªä¸åŒworkerçš„æ¢¯åº¦ã€‚ConditionalAccumulatorå†…éƒ¨å®é™…ä¸Šç»´æŠ¤äº†ä¸€ä¸ªtime stepï¼Œè®°å½•äº†å½“å‰é›†ç¾¤è®­ç»ƒçš„è¿›åº¦ã€‚åœ¨Accumulatorä¸­æä¾›äº†<code class="highlighter-rouge">apply_grad()</code>æ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•éœ€è¦å°†<code class="highlighter-rouge">local_step</code>ä½œä¸ºä¸€ä¸ªå‚æ•°ä¼ å…¥ï¼Œè¿”å›ä¸€ä¸ªOperationï¼Œå¦‚æœ<code class="highlighter-rouge">local_step</code>å°äºå†…éƒ¨çš„time stepï¼Œé‚£è¿™ä¸ªOperationå°±ä¸ä¼šè¢«æ‰§è¡Œã€‚è€Œ<code class="highlighter-rouge">take_grad()</code>æ–¹æ³•åœ¨Accumulatorå†…éƒ¨æ”¶é›†çš„gradientæ•°ç›®å°‘äº<code class="highlighter-rouge">num_required</code>å‚æ•°æŒ‡å‡ºçš„æ•°ç›®æ—¶ï¼Œå°±ä¼šé˜»å¡ï¼Œå®ƒè¢«è°ƒç”¨å®Œä¹‹åï¼ŒAccumulatorå†…éƒ¨çš„gradientè®¡æ•°å™¨ä¼šæ¸…é›¶ï¼ŒåŒæ—¶å†…éƒ¨çš„time stepä¼šé€’å¢1ä¸ªå•ä½ã€‚</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      <span class="c1"># sync_op will be assigned to the same device as the global step.
</span>      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">global_step</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">""</span><span class="p">):</span>
        <span class="n">update_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">aggregated_grads_and_vars</span><span class="p">,</span>
                                              <span class="n">global_step</span><span class="p">)</span>  <span class="c1"># çœŸæ­£å®Œæˆå‚æ•°æ›´æ–°çš„åœ°æ–¹ï¼Œæ‰§è¡Œä¹‹åï¼Œglobal_stepä¼š+1
</span>
      <span class="c1"># Create token queue.
</span>      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">global_step</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">""</span><span class="p">):</span>
        <span class="n">sync_token_queue</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">data_flow_ops</span><span class="o">.</span><span class="n">FIFOQueue</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                                    <span class="n">global_step</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span>
                                    <span class="n">shapes</span><span class="o">=</span><span class="p">(),</span>
                                    <span class="n">name</span><span class="o">=</span><span class="s">"sync_token_q"</span><span class="p">,</span>
                                    <span class="n">shared_name</span><span class="o">=</span><span class="s">"sync_token_q"</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_token_queue</span> <span class="o">=</span> <span class="n">sync_token_queue</span>

        <span class="c1"># dummy_queue is passed to the queue runner. Don't use the real queues
</span>        <span class="c1"># because the queue runner doesn't automatically reopen it once it
</span>        <span class="c1"># closed queues in PS devices.
</span>        <span class="n">dummy_queue</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">data_flow_ops</span><span class="o">.</span><span class="n">FIFOQueue</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>
                                    <span class="n">types_pb2</span><span class="o">.</span><span class="n">DT_INT32</span><span class="p">,</span>
                                    <span class="n">shapes</span><span class="o">=</span><span class="p">(),</span>
                                    <span class="n">name</span><span class="o">=</span><span class="s">"dummy_queue"</span><span class="p">,</span>
                                    <span class="n">shared_name</span><span class="o">=</span><span class="s">"dummy_queue"</span><span class="p">))</span>

      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">global_step</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">""</span><span class="p">):</span>
        <span class="c1"># Replicas have to wait until they can get a token from the token queue.
</span>        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="n">train_ops</span><span class="p">):</span>
          <span class="n">token</span> <span class="o">=</span> <span class="n">sync_token_queue</span><span class="o">.</span><span class="n">dequeue</span><span class="p">()</span>  <span class="c1"># è¿™ä¸ªOperationåªä¾èµ–ä¸train_opsï¼Œè€Œä¸ä¾èµ–äºupdate_op
</span>        <span class="n">train_op</span> <span class="o">=</span> <span class="n">state_ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_local_step</span><span class="p">,</span> <span class="n">token</span><span class="p">)</span>  <span class="c1"># æ›´æ–°_local_stepçš„å€¼ä¸ºé›†ç¾¤æ‰€è¦æ±‚çš„step
</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">update_op</span><span class="p">]):</span>
          <span class="c1"># Sync_op needs to insert tokens to the token queue at the end of the
</span>          <span class="c1"># step so the replicas can fetch them to start the next step.
</span>          <span class="n">tokens</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_tokens_per_step</span><span class="p">],</span> <span class="n">global_step</span><span class="p">)</span>
          <span class="n">sync_op</span> <span class="o">=</span> <span class="n">sync_token_queue</span><span class="o">.</span><span class="n">enqueue_many</span><span class="p">((</span><span class="n">tokens</span><span class="p">,))</span>  <span class="c1"># å°†æ–°çš„toeknsæ”¾å…¥é˜Ÿåˆ—ä¸­ï¼Œtokensæ˜¯æ–°çš„global_step
</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable_averages</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
          <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">sync_op</span><span class="p">]),</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">""</span><span class="p">):</span>
            <span class="n">sync_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable_averages</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_variables_to_average</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_chief_queue_runner</span> <span class="o">=</span> <span class="n">queue_runner</span><span class="o">.</span><span class="n">QueueRunner</span><span class="p">(</span><span class="n">dummy_queue</span><span class="p">,</span>
                                                            <span class="p">[</span><span class="n">sync_op</span><span class="p">])</span>
      <span class="k">for</span> <span class="n">accum</span><span class="p">,</span> <span class="n">dev</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_accumulator_list</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">dev</span><span class="p">):</span>
          <span class="n">chief_init_ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
              <span class="n">accum</span><span class="o">.</span><span class="n">set_global_step</span><span class="p">(</span>
                  <span class="n">global_step</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"SetGlobalStep"</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">chief_init_op</span> <span class="o">=</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">chief_init_ops</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_gradients_applied</span> <span class="o">=</span> <span class="bp">True</span>
      <span class="k">return</span> <span class="n">train_op</span>
</code></pre></div></div>

<p>ä¸Šé¢çš„æµç¨‹è¯´æ˜äº†å¦‚ä½•é€šè¿‡<code class="highlighter-rouge">token_queue</code>æ§åˆ¶å„ä¸ªworkerçš„è®¡ç®—è¿›åº¦ã€‚åœ¨ä¸Šé¢ä»£ç ç»™å‡ºçš„æµç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å°†tokençœ‹åšè®¡ç®—é›†ç¾¤å½“å‰è¦æ±‚å„ä¸ªworkerå®Œæˆçš„ä»»åŠ¡ï¼Œå¦‚æœworkerè·å¾—äº†tokenï¼Œé‚£ä¹ˆä»£è¡¨ç€è¿™ä¸ªworkeré¢†å–äº†tokenè¡¨ç¤ºçš„<code class="highlighter-rouge">global_step</code>çš„ä»»åŠ¡ã€‚<code class="highlighter-rouge">token_queue</code>çš„æ›´æ–°æ˜¯éœ€è¦ç­‰å¾…çœŸå®çš„Optimizerå®Œæˆäº†å‚æ•°çš„æ›´æ–°ä¹‹åï¼Œæ‰ä¼šå‘token queueä¸­æ”¾å…¥ä¸€å®šæ•°ç›®çš„æ–°çš„tokenã€‚</p>

<p>æœ‰ä¸€ç‚¹éœ€è¦æ³¨æ„ï¼Œå–tokenè¿™ä¸ªOperationåªä¾èµ–äº<code class="highlighter-rouge">train_ops</code>ï¼Œè€Œ<code class="highlighter-rouge">train_ops</code>è¡¨ç¤ºç€åœ¨Accumulatorä¸­çš„apply_gradientsçš„Operationï¼Œå¯ä»¥ç®€å•çš„æŠŠ<code class="highlighter-rouge">train_ops</code>çœ‹åšpushçš„è¿‡ç¨‹ã€‚è¿™æ„å‘³ç€ï¼Œå½“workerå®Œæˆäº†ä¸€è½®æ¢¯åº¦è®¡ç®—ä¹‹åï¼Œå¦‚æœ<code class="highlighter-rouge">token_queue</code>ä¸­è¿˜æœ‰å‰©ä½™çš„tokenï¼Œé‚£ä¹ˆå®ƒå°†ä¼šé¢†å–ä¸€ä¸ªå‰©ä½™çš„tokenï¼Œåˆ©ç”¨ä¸‹ä¸€ä¸ªbatchçš„æ•°æ®å¼€å§‹ä¸€è½®ç›¸åŒçš„è¿­ä»£ã€‚</p>

<p>è‡³æ­¤ï¼Œ<code class="highlighter-rouge">sync_replicas_optimizer</code>ä¸­æœ€é‡è¦çš„ï¼Œåˆ©ç”¨å¹³å‡åçš„æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°çš„æ–¹æ³•<code class="highlighter-rouge">apply_gradients()</code>çš„æ‰§è¡Œæµç¨‹ï¼Œåº”è¯¥å·²ç»æ¯”è¾ƒæ¸…æ¥šäº†ï¼Œæ€»ç»“ä¸€ä¸‹ï¼Œæ¯”è¾ƒé‡è¦çš„æ˜¯æœ‰ä»¥ä¸‹å‡ ä¸ªç‚¹ï¼š</p>
<ol>
  <li>ConditionalAccumulatorçš„æ¢¯åº¦èšåˆ
ConditionalAccumulatorå¯ä»¥èšåˆé‚£äº›åˆé€‚çš„æ¢¯åº¦ï¼Œstaleçš„æ¢¯åº¦å€¼ä¼šè¢«è‡ªåŠ¨æŠ›å¼ƒï¼Œä¸”ç›´åˆ°æ”¶é›†åˆ°è¶³å¤Ÿçš„æ¢¯åº¦ä¹‹åï¼Œæ‰èƒ½è·å¾—å¹³å‡æ¢¯åº¦ã€‚</li>
  <li>Tokenæ§åˆ¶è®­ç»ƒè¿›åº¦
åœ¨<a href="https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer#get_init_tokens_op">å®˜æ–¹æ–‡æ¡£</a>ä¸­ï¼Œå¯¹Tokenæåˆ°çš„å…³é”®ä½œç”¨åœ¨äºï¼Œå½“workeræ•°ç›®å°‘äºæ¯ä¸€è½®è¿­ä»£éœ€è¦æ”¶é›†çš„æ¢¯åº¦(<code class="highlighter-rouge">total_num_replicas</code>  &lt;  ` replicas_to_aggregate`)
æ—¶ï¼Œè®­ç»ƒè¿›ç¨‹èƒ½å¤Ÿæ­£å¸¸è¿­ä»£ä¸‹å»ã€‚ä½†æ˜¯é™¤äº†æ–‡æ¡£æåˆ°çš„è¿™ä¸ªç‰¹æ€§ä¹‹å¤–ï¼Œåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹å¼€å§‹ä¹‹å‰ï¼Œåˆå§‹çš„tokenæ•°ç›®å°†ä¼šå½±å“åˆ°å„ä¸ªworkerçš„è®­ç»ƒè¿›åº¦ã€‚ä¸‹é¢å°†å…ˆçœ‹ä¸€ä¸‹åˆå§‹tokenæ•°ç›®çš„è®¾ç½®ï¼Œå†åˆ†ætokenæ•°ç›®å¯¹è®­ç»ƒè¿›åº¦äº§ç”Ÿå½±å“çš„åŸå› ã€‚</li>
</ol>

<h5 id="get_init_tokens_op"><code class="highlighter-rouge">get_init_tokens_op</code></h5>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">get_init_tokens_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_tokens</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradients_applied</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">:</span>
      <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span>
          <span class="s">"get_init_tokens_op() should be called after apply_gradients()."</span><span class="p">)</span>

    <span class="n">tokens_needed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replicas_to_aggregate</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_num_replicas</span>
    <span class="k">if</span> <span class="n">num_tokens</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
      <span class="n">num_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replicas_to_aggregate</span>
    <span class="k">elif</span> <span class="n">num_tokens</span> <span class="o">&lt;</span> <span class="n">tokens_needed</span><span class="p">:</span>
      <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span>
          <span class="s">"Too few tokens to finish the first step: </span><span class="si">%</span><span class="s">d (given) vs </span><span class="si">%</span><span class="s">d (needed)"</span> <span class="o">%</span>
          <span class="p">(</span><span class="n">num_tokens</span><span class="p">,</span> <span class="n">tokens_needed</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">num_tokens</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">""</span><span class="p">):</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">num_tokens</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">)</span>
        <span class="n">init_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sync_token_queue</span><span class="o">.</span><span class="n">enqueue_many</span><span class="p">((</span><span class="n">tokens</span><span class="p">,))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">init_tokens</span> <span class="o">=</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">no_op</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"no_init_tokens"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">init_tokens</span>

  <span class="k">def</span> <span class="nf">make_session_run_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_chief</span><span class="p">,</span> <span class="n">num_tokens</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
    <span class="s">"""Creates a hook to handle SyncReplicasHook ops such as initialization."""</span>
    <span class="k">return</span> <span class="n">_SyncReplicasOptimizerHook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_chief</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">)</span>
</code></pre></div></div>

<p>è¿™éƒ¨åˆ†çš„ä»£ç ç»™å‡ºäº†åœ¨è®­ç»ƒå¼€å§‹ä¹‹å‰ï¼Œå‘token_queueæ”¾å…¥å¤šå°‘tokençš„æ‰§è¡Œæµç¨‹ã€‚è¿™ä¸€éƒ¨åˆ†çš„ä»£ç ä¼šé€šè¿‡hookçš„æ–¹å¼ï¼Œåœ¨sessionåˆ›å»ºä¹‹åï¼Œå¯¹æ¨¡å‹å‚æ•°åˆå§‹åŒ–æ—¶æ‰§è¡Œã€‚</p>

<p>åœ¨é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœä¸äººä¸ºæŒ‡å®šå‚æ•°ï¼Œä¼ å…¥çš„å‚æ•°<code class="highlighter-rouge">num_tokens</code>ä¸º<code class="highlighter-rouge">-1</code>ï¼Œæ­¤æ—¶å°†ä¼šåœ¨<code class="highlighter-rouge">token_queue</code>ä¸­æ”¾å…¥<code class="highlighter-rouge">replicas_to_aggregate</code>ä¸ªtokenã€‚åœ¨å®é™…å®éªŒè¿‡ç¨‹ä¸­ï¼Œè¿™å°†ä¼šå¯¼è‡´ä¸€äº›ä¸åŒæ­¥çš„é—®é¢˜ï¼Œåœ¨issues <a href="https://github.com/tensorflow/tensorflow/issues/11753#issuecomment-377855509">#11753</a>ä¸­æåˆ°äº†è§£å†³æ–¹æ³•ï¼Œæ˜¯å°†åˆå§‹tokenæ•°ç›®è®¾ç½®ä¸º0å³å¯ã€‚æˆ‘é€šè¿‡å®éªŒä¹ŸéªŒè¯äº†è¿™ä¸ªæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>

<p>ä¸Šé¢çš„<code class="highlighter-rouge">apply_gradients</code>çš„æ‰§è¡Œæµç¨‹ï¼Œå¯ä»¥è§£é‡Šè¿™ä¸ªç°è±¡å‘ç”Ÿçš„åŸå› ã€‚å› ä¸ºå½“workerå®Œæˆäº†å‘å‚æ•°æœåŠ¡å™¨ push æ¢¯åº¦çš„è¿‡ç¨‹ä¹‹åï¼Œå°±ä¼šç”³è¯·dequeueï¼Œå¦‚æœæ­¤æ—¶<code class="highlighter-rouge">token_queue</code>ä¸­æœ‰tokenï¼Œé‚£ä¹ˆworkerå°±èƒ½å–æ–°çš„è®­ç»ƒæ•°æ®ï¼Œä¾ç„¶åˆ©ç”¨åˆšæ‰çš„æ¨¡å‹å‚æ•°å†è®¡ç®—ä¸€è½®æ¢¯åº¦ã€‚å®é™…ä¸Šï¼Œå¦‚æœå„ä¸ªworkerç®—åŠ›å·®åˆ«å¾ˆå¤§ï¼Œæ˜¯å¯èƒ½å‡ºç°K-batch-sync SGDè¿™ç§è®­ç»ƒæ¨¡å¼çš„ï¼Œä¸‹å›¾å¾ˆå¥½çš„è¯´æ˜äº†è¿™ä¸€ç‚¹ã€‚ä¸‹å›¾çš„è®¾ç½®å¯ä»¥ç†è§£ä¸º<code class="highlighter-rouge">total_num_replicas=3</code>,<code class="highlighter-rouge">replicas_to_aggregate=2</code>,çš„æƒ…å†µã€‚L2çš„è®¡ç®—é€Ÿåº¦ç›¸è¾ƒäºå¦å¤–ä¸¤ä¸ªworkerå¿«å¾ˆå¤šï¼Œç”±äºå®ƒæœ¬åœ°è¿­ä»£ä¸€è½®ä¹‹åï¼Œå¯ä»¥ä»queueä¸­å–å‡ºä¸€ä¸ªtokenï¼Œå¼€å§‹æ–°çš„ä¸€è½®è®¡ç®—ã€‚æœ¬åœ°è¿­ä»£ä¸¤è½®ä¹‹åï¼Œå¦å¤–ä¸¤ä¸ªworkerè¿˜æœªè¿­ä»£å®Œä¸€è½®ï¼Œè¿™æ—¶ï¼Œå‚æ•°æœåŠ¡å™¨å°†ä¼šåˆ©ç”¨L2çš„ä¸¤æ¬¡æ¢¯åº¦è¿›è¡Œå¹³å‡ï¼Œå¯¹å‚æ•°æ›´æ–°ä¹‹åï¼Œ<code class="highlighter-rouge">global_step</code>é€’å¢1ï¼Œå¼€å§‹å…¨å±€çš„æ–°ä¸€è½®è®¡ç®—ã€‚
<img src="/img/1567651943862.png" alt="@K-batch-sync SGD" /></p>
<h3 id="å‚è€ƒèµ„æ–™">å‚è€ƒèµ„æ–™</h3>
<ol>
  <li><a href="https://github.com/tensorflow/tensorflow/issues/11753">tf.train.SyncReplicasOptimizer no synchronization among workers #11753</a></li>
  <li><a href="https://github.com/tensorflow/tensorflow/issues/9596">Synchronous distributed tensorflow training doesnâ€™t synchronize among workers #9596</a></li>
  <li><a href="tf.train.SyncReplicasOptimizer">tf.train.SyncReplicasOptimizer</a></li>
  <li><a href="https://zhuanlan.zhihu.com/p/40342278">Optimizer in Tensorflow</a></li>
  <li><a href="https://arxiv.org/abs/1803.01113">Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD</a></li>
</ol>

:ET