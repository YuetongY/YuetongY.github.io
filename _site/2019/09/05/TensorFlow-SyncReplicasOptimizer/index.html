<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="1">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>TensorFlow SyncReplicasOptimizer 解读 - Yuetong's Log</title>

    <link rel="canonical" href="http://localhost:4000/2019/09/05/TensorFlow-SyncReplicasOptimizer/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="http://cdn.staticfile.org/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Logging.debug</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
                    
                    <li>
                        <a href="/about/">About</a>
                    </li>
                    
                    <li>
                        <a href="/tags/">Tags</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Image to hack wechat -->
<!-- <img src="/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="/img/ps.png" width="0" height="0"> -->

<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        background-image: url('/img/ps.png')
    }
</style>
<header class="intro-header" >
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/#TensorFlow" title="TensorFlow">TensorFlow</a>
                        
                        <a class="tag" href="/tags/#深度学习" title="深度学习">深度学习</a>
                        
                        <a class="tag" href="/tags/#分布式训练" title="分布式训练">分布式训练</a>
                        
                        <a class="tag" href="/tags/#参数服务器" title="参数服务器">参数服务器</a>
                        
                    </div>
                    <h1>TensorFlow SyncReplicasOptimizer 解读</h1>
                    
                    
                    <h2 class="subheading">对SyncReplicasOptimizer实现的分布式训练的分析</h2>
                    
                    <span class="meta">Posted by yuetong on September 5, 2019</span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

				<h3 id="背景">背景</h3>
<p>在CNN等深度神经网络的分布式训练中，比较常用的一种训练模式是（同步的）数据并行，也就是各个计算设备分别根据各自获得的batch，前向计算获得损失，进而反向传播计算梯度。待所有计算设备完成梯度计算之后，对梯度进行平均，利用平均梯度对模型进行更新。</p>

<p>上面这种模式只是一种逻辑架构，具体实现上，可以使用参数服务器（PS）的形式实现。在参数服务器架构中，计算设备被划分为参数服务器(ps)和worker。对于ps，顾名思义，主要是对模型的参数进行存储；而对于worker，主要的工作是完成前向和反向传播这类计算密集的运算。</p>

<p>在（同步的）参数服务器架构中，整个流程可以分为以下几个步骤：</p>
<ol>
  <li>worker从ps把模型参数pull到本地的memory中</li>
  <li>worker利用模型参数完成前向计算，再完成反向传播，得到模型参数的梯度</li>
  <li>worker将模型参数的梯度push至参数服务器，参数服务器收到所有worker的梯度之后，计算平均梯度，并对模型参数进行更新</li>
</ol>

<p>之所以想到查看<code class="highlighter-rouge">SyncReplicasOptimizer</code>的实现，是因为我在实验中，用到了这个Optimizer，最初看它的名字，想当然的认为它是一个同步的Optimizer，但是实际使用发现如果参数设置不当，可能会出现各个worker计算进度不一致，也就是没有真正的同步。另外，在实验中发现在我的实验环境下，PS架构 + <code class="highlighter-rouge">sync_replicas_optimizer</code>不能完全发挥集群的性能，可能需要了解sync_replicas_optimizer的工作原理才能更好的发现问题所在。</p>

<p>下面的介绍是我个人查看<code class="highlighter-rouge">tf.train.Sync_replicas_optimizer</code>实现部分的理解。</p>

<h3 id="syncreplicasoptimizer解析">SyncReplicasOptimizer解析</h3>
<h4 id="文档说明">文档说明</h4>
<p>在TensorFlow r1.14版本的<a href="https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer">文档</a>中，这个Optimizer提供的API已经被弃用了，在新的API中，如果希望在分布式环境中实现同步的训练，是通过配置<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute">Distribution Strategies</a>来实现的。</p>
<blockquote>
  <p>This class is deprecated. For synchrononous training, please use <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute">Distribution Strategies</a>.</p>
</blockquote>

<p>在文档中，给出了这个优化器的“同步”训练方案，如下：</p>
<blockquote>
  <ul>
    <li>For the Parameter Server job:
      <ol>
        <li>An accumulator is created for each variable, and each replica pushes the gradients into the accumulators instead of directly applying them to the variables.</li>
        <li>Each accumulator averages once enough gradients (replicas_to_aggregate) have been accumulated.</li>
        <li>Apply the averaged gradients to the variables.</li>
        <li>Only after all variables have been updated, increment the global step.</li>
        <li>Only after step 4, pushes global_step in the token_queue, once for each worker replica. The workers can now fetch the global step, use it to update its local_step variable and start the next batch. Please note that some workers can consume multiple minibatches, while some may not consume even one. This is because each worker fetches minibatches as long as a token exists. If one worker is stuck for some reason and does not consume a token, another worker can use it.</li>
      </ol>
    </li>
    <li>For the replicas:
      <blockquote>
        <ol>
          <li>Start a step: fetch variables and compute gradients.</li>
          <li>Once the gradients have been computed, push them into gradient accumulators. Each accumulator will check the staleness and drop the stale.</li>
          <li>After pushing all the gradients, dequeue an updated value of global_step from the token queue and record that step to its local_step variable. Note that this is effectively a barrier.</li>
          <li>Start the next batch.</li>
        </ol>
      </blockquote>
    </li>
  </ul>
</blockquote>

<p>在文档中，Parameter Server job可以简单理解为参数服务器，而Replicas可以简单理解为各个worker，或者是各个GPU。可以从文档中得到这两个信息：</p>
<ul>
  <li>参数服务器是通过为每一个variable建立一个<code class="highlighter-rouge">accumulator</code>数据结构，进而对来自不同worker的梯度进行管理（平均）的</li>
  <li><code class="highlighter-rouge">token_queue</code>这个数据结构被用来保持各个worker的进度（一致）</li>
</ul>

<p>文档中的叙述展示了这个Optimizer的执行逻辑，但是对于TensorFlow而言，它是通过计算图完成计算的，上面的流程如何在计算图中得到体现，就需要查看<code class="highlighter-rouge">sync_replicas_optimizer</code>的实现部分。</p>
<h4 id="源码部分">源码部分</h4>
<h5 id="__init__"><code class="highlighter-rouge">__init__</code></h5>
<p><code class="highlighter-rouge">sync_replicas_optimizer</code>是一个wrapper optimizer，也就是这个Optimizer对其他Optimizer进行包装，完成worker间梯度同步的工作，而实际梯度的计算则是交由被包装的Optimizer来完成的。这一点可以通过构造函数发现：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">opt</span><span class="p">,</span>
               <span class="n">replicas_to_aggregate</span><span class="p">,</span>
               <span class="n">total_num_replicas</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">variable_averages</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">variables_to_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">use_locking</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s">"sync_replicas"</span><span class="p">):</span>
               <span class="k">if</span> <span class="n">total_num_replicas</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
				    <span class="n">total_num_replicas</span> <span class="o">=</span> <span class="n">replicas_to_aggregate</span>
			    <span class="nb">super</span><span class="p">(</span><span class="n">SyncReplicasOptimizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">use_locking</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
			    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
			        <span class="s">"SyncReplicasV2: replicas_to_aggregate=</span><span class="si">%</span><span class="s">s; total_num_replicas=</span><span class="si">%</span><span class="s">s"</span><span class="p">,</span>
			        <span class="n">replicas_to_aggregate</span><span class="p">,</span> <span class="n">total_num_replicas</span><span class="p">)</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_opt</span> <span class="o">=</span> <span class="n">opt</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_replicas_to_aggregate</span> <span class="o">=</span> <span class="n">replicas_to_aggregate</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_gradients_applied</span> <span class="o">=</span> <span class="bp">False</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_variable_averages</span> <span class="o">=</span> <span class="n">variable_averages</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_variables_to_average</span> <span class="o">=</span> <span class="n">variables_to_average</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_total_num_replicas</span> <span class="o">=</span> <span class="n">total_num_replicas</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_tokens_per_step</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">total_num_replicas</span><span class="p">,</span> <span class="n">replicas_to_aggregate</span><span class="p">)</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span> <span class="o">=</span> <span class="bp">None</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_sync_token_queue</span> <span class="o">=</span> <span class="bp">None</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_chief_queue_runner</span> <span class="o">=</span> <span class="bp">None</span>
			    <span class="bp">self</span><span class="o">.</span><span class="n">_accumulator_list</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></div>

<p>在构造函数中，可以传入这么几个参数，<code class="highlighter-rouge">opt</code>,<code class="highlighter-rouge">replicas_to_aggregate</code>,<code class="highlighter-rouge">total_num_replicas</code>,<code class="highlighter-rouge">variable_averages</code>,<code class="highlighter-rouge">variables_to_average</code>,<code class="highlighter-rouge">use_locking</code>,<code class="highlighter-rouge">name</code>。各个参数具体的含义可以参考<a href="https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer#__init__">官方文档</a>给出的解释。需要说明的有几个点，<code class="highlighter-rouge">opt</code>这个参数体现了wrapper的思想，它是一个<code class="highlighter-rouge">Optimizer</code>类的一个对象，通过它来真正完成梯度的计算。<code class="highlighter-rouge">total_num_replicas</code>和<code class="highlighter-rouge">replicas_to_aggregate</code>这两个参数会被用来控制同步的进度，前者表示<strong>集群中worker的数目</strong>，而后者表示<strong>ps完成一次参数更新所需要收集的梯度数目</strong>。在同步的训练模式下，它们两个应当是相等的，但是事实上，即使它们俩相等，也有可能导致不同步的现象出现，这与其他参数(<code class="highlighter-rouge">token_num</code>)的设置有关系。</p>

<p>如果前者比后者大，逻辑上意味着ps在一轮迭代中，只需要收集到部分梯度，就利用这些梯度计算平均梯度，进而对参数更新，这可能意味着这轮迭代中，计算较慢的那个节点的梯度会被丢弃<em>（只是一种猜测，还未实验验证）</em>。这两个参数与之前提到的<code class="highlighter-rouge">token_queue</code>的设置有关系，在后面的源码介绍将会更加详细的说明。</p>

<p>在构造函数中，还为这个Optimizer定义了一些新的数据结构，就不详细介绍了，读完整个执行流程之后，它们的功能应该就会比较清楚。</p>

<h5 id="compute_gradients"><code class="highlighter-rouge">compute_gradients</code></h5>
<p>下面来看<code class="highlighter-rouge">compute_gradients</code>的实现。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="k">def</span> <span class="nf">compute_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
	   <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_opt</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></div>

<p>可以从源码中验证wrapper的思想，梯度的计算是通过被包装的Optimizer来实现的。</p>

<h5 id="apply_gradients"><code class="highlighter-rouge">apply_gradients</code></h5>
<p>这个方法将会构造模型参数更新部分的计算图，最后将返回一个<code class="highlighter-rouge">train_op</code>。<code class="highlighter-rouge">train_op</code>是通常训练过程中，client为session的fetches提供的参数之一，也就是这个Operation被执行之后，模型的参数将会完成更新，并开始下一个batch的训练。那么这也就意味着，这个方法中涉及到的计算图将会实现说明文档中的训练逻辑。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">apply_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads_and_vars</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">grads_and_vars</span><span class="p">:</span>
      <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Must supply at least one variable"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">global_step</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Global step is required to check staleness"</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span> <span class="o">=</span> <span class="n">global_step</span>
    <span class="n">train_ops</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 所有的train_op，也就是Accumulator的apply_gradients，实际上可以指push的过程
</span>    <span class="n">aggregated_grad</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 平均后的梯度
</span>    <span class="n">var_list</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 变量存放的列表
</span>    <span class="o">...</span>
</code></pre></div></div>

<p>上面的代码负责验证传入方法的参数的合法性，并创建一些供方法内部使用的数据结构。传入的参数必须包含有<code class="highlighter-rouge">global_step</code>，这在其他Optimizer可能不是一个必须的参数，在这里，它为必须的，因为它需要被用在检查stale gradiets的地方。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">local_anchor</span> <span class="o">=</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">no_op</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="n">local_anchor</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_local_step</span> <span class="o">=</span> <span class="n">variable_scope</span><span class="o">.</span><span class="n">variable</span><span class="p">(</span>
          <span class="n">initial_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
          <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="p">[</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">LOCAL_VARIABLES</span><span class="p">],</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">global_step</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="s">"sync_rep_local_step"</span><span class="p">)</span>
      <span class="c1"># _local_step的初始化Operation
</span>      <span class="bp">self</span><span class="o">.</span><span class="n">local_step_init_op</span> <span class="o">=</span> <span class="n">state_ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_local_step</span><span class="p">,</span> <span class="n">global_step</span><span class="p">)</span>
      <span class="n">chief_init_ops</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">local_step_init_op</span><span class="p">]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">ready_for_local_init_op</span> <span class="o">=</span> <span class="n">variables</span><span class="o">.</span><span class="n">report_uninitialized_variables</span><span class="p">(</span>
        <span class="n">variables</span><span class="o">.</span><span class="n">global_variables</span><span class="p">())</span>
</code></pre></div></div>

<p>这一部分首先在计算图上创建了一个空的Operation，之后创建了一个类的成员变量<code class="highlighter-rouge">_local_step</code>，用来记录这个worker的计算进度。用到<code class="highlighter-rouge">ops.colocate_with()</code>的原因是在于，在一般情况下，PS架构会通过device function等机制，将这类Operation放在worker所处的device上，而将Variables这类特殊的Operation放在PS上。<code class="highlighter-rouge">local_anchor</code>是一个一般的Operation，因此它会被PS架构分配至worker所处的device，这样就能确保创建的<code class="highlighter-rouge">_local_step</code>变量也能够放在worker上。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">):</span>  <span class="c1"># 在计算图中创建以这个优化器为名称的name_scope
</span>      <span class="k">for</span> <span class="n">grad</span><span class="p">,</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">grads_and_vars</span><span class="p">:</span>  <span class="c1"># 遍历计算好的每一份梯度
</span>        <span class="n">var_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>  <span class="c1"># 将variables按顺序保存在一个列表里
</span>        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>  <span class="c1"># 下面定义的计算图的部分将与variables放在相同的device上。可以理解为都被放在PS上
</span>          <span class="c1"># Dense gradients.
</span>          <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">aggregated_grad</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span>  <span class="c1"># pass-through.
</span>            <span class="k">continue</span>
          <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="c1"># 为每一份梯度创建一个ConditionalAccumulator
</span>            <span class="n">grad_accum</span> <span class="o">=</span> <span class="n">data_flow_ops</span><span class="o">.</span><span class="n">ConditionalAccumulator</span><span class="p">(</span>
                <span class="n">grad</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">get_shape</span><span class="p">(),</span>
                <span class="n">shared_name</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s">"/grad_accum"</span><span class="p">)</span>
            <span class="c1"># 在train_ops这个集合中放入对应accumulator的apply_grad operation
</span>            <span class="c1"># 注意，这个apply_grad Operation传入了一个local_step参数，这个操Operation是把梯度放到accumulator中，如果local_step落后于global_step，这个Operation会自动终止，也就是梯度不会放入accumulator
</span>            <span class="n">train_ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_accum</span><span class="o">.</span><span class="n">apply_grad</span><span class="p">(</span>
                <span class="n">grad</span><span class="p">,</span> <span class="n">local_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_local_step</span><span class="p">))</span>
            <span class="c1"># aggregrated_grad中有所有平均之后的梯度，它是从accumulator中执行take_grad的Operation
</span>            <span class="c1"># take_grad()中有一个参数，num_required，如果说accumulator中的梯度数量少于num_required，它将会阻塞。
</span>            <span class="n">aggregated_grad</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_accum</span><span class="o">.</span><span class="n">take_grad</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_replicas_to_aggregate</span><span class="p">))</span>
          <span class="k">else</span><span class="p">:</span>
		    <span class="c1"># 稀疏梯度的处理，与上面类似
</span>            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
              <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Unknown grad type!"</span><span class="p">)</span>
            <span class="n">grad_accum</span> <span class="o">=</span> <span class="n">data_flow_ops</span><span class="o">.</span><span class="n">SparseConditionalAccumulator</span><span class="p">(</span>
                <span class="n">grad</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">shared_name</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s">"/grad_accum"</span><span class="p">)</span>
            <span class="n">train_ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_accum</span><span class="o">.</span><span class="n">apply_indexed_slices_grad</span><span class="p">(</span>
                <span class="n">grad</span><span class="p">,</span> <span class="n">local_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_local_step</span><span class="p">))</span>
            <span class="n">aggregated_grad</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_accum</span><span class="o">.</span><span class="n">take_indexed_slices_grad</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_replicas_to_aggregate</span><span class="p">))</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">_accumulator_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">grad_accum</span><span class="p">,</span> <span class="n">var</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
      <span class="n">aggregated_grads_and_vars</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">aggregated_grad</span><span class="p">,</span> <span class="n">var_list</span><span class="p">)</span>  <span class="c1"># 最终的(平均梯度，参数)列表
</span></code></pre></div></div>

<p>上面的代码给出的是进行梯度聚合，最终获得对应每一个参数的平均梯度的过程。在上面这个过程中，分别为每一个Variable创建了一个<code class="highlighter-rouge">ConditionalAccumulator</code>，<code class="highlighter-rouge">ConditionalAccumulator</code>是被多个Session所共享的，用于维护在这个time step下来自不同worker的梯度。ConditionalAccumulator内部实际上维护了一个time step，记录了当前集群训练的进度。在Accumulator中提供了<code class="highlighter-rouge">apply_grad()</code>方法，这个方法需要将<code class="highlighter-rouge">local_step</code>作为一个参数传入，返回一个Operation，如果<code class="highlighter-rouge">local_step</code>小于内部的time step，那这个Operation就不会被执行。而<code class="highlighter-rouge">take_grad()</code>方法在Accumulator内部收集的gradient数目少于<code class="highlighter-rouge">num_required</code>参数指出的数目时，就会阻塞，它被调用完之后，Accumulator内部的gradient计数器会清零，同时内部的time step会递增1个单位。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      <span class="c1"># sync_op will be assigned to the same device as the global step.
</span>      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">global_step</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">""</span><span class="p">):</span>
        <span class="n">update_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">aggregated_grads_and_vars</span><span class="p">,</span>
                                              <span class="n">global_step</span><span class="p">)</span>  <span class="c1"># 真正完成参数更新的地方，执行之后，global_step会+1
</span>
      <span class="c1"># Create token queue.
</span>      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">global_step</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">""</span><span class="p">):</span>
        <span class="n">sync_token_queue</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">data_flow_ops</span><span class="o">.</span><span class="n">FIFOQueue</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                                    <span class="n">global_step</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span>
                                    <span class="n">shapes</span><span class="o">=</span><span class="p">(),</span>
                                    <span class="n">name</span><span class="o">=</span><span class="s">"sync_token_q"</span><span class="p">,</span>
                                    <span class="n">shared_name</span><span class="o">=</span><span class="s">"sync_token_q"</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_token_queue</span> <span class="o">=</span> <span class="n">sync_token_queue</span>

        <span class="c1"># dummy_queue is passed to the queue runner. Don't use the real queues
</span>        <span class="c1"># because the queue runner doesn't automatically reopen it once it
</span>        <span class="c1"># closed queues in PS devices.
</span>        <span class="n">dummy_queue</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">data_flow_ops</span><span class="o">.</span><span class="n">FIFOQueue</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>
                                    <span class="n">types_pb2</span><span class="o">.</span><span class="n">DT_INT32</span><span class="p">,</span>
                                    <span class="n">shapes</span><span class="o">=</span><span class="p">(),</span>
                                    <span class="n">name</span><span class="o">=</span><span class="s">"dummy_queue"</span><span class="p">,</span>
                                    <span class="n">shared_name</span><span class="o">=</span><span class="s">"dummy_queue"</span><span class="p">))</span>

      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">global_step</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">""</span><span class="p">):</span>
        <span class="c1"># Replicas have to wait until they can get a token from the token queue.
</span>        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="n">train_ops</span><span class="p">):</span>
          <span class="n">token</span> <span class="o">=</span> <span class="n">sync_token_queue</span><span class="o">.</span><span class="n">dequeue</span><span class="p">()</span>  <span class="c1"># 这个Operation只依赖与train_ops，而不依赖于update_op
</span>        <span class="n">train_op</span> <span class="o">=</span> <span class="n">state_ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_local_step</span><span class="p">,</span> <span class="n">token</span><span class="p">)</span>  <span class="c1"># 更新_local_step的值为集群所要求的step
</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">update_op</span><span class="p">]):</span>
          <span class="c1"># Sync_op needs to insert tokens to the token queue at the end of the
</span>          <span class="c1"># step so the replicas can fetch them to start the next step.
</span>          <span class="n">tokens</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_tokens_per_step</span><span class="p">],</span> <span class="n">global_step</span><span class="p">)</span>
          <span class="n">sync_op</span> <span class="o">=</span> <span class="n">sync_token_queue</span><span class="o">.</span><span class="n">enqueue_many</span><span class="p">((</span><span class="n">tokens</span><span class="p">,))</span>  <span class="c1"># 将新的toekns放入队列中，tokens是新的global_step
</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable_averages</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
          <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">sync_op</span><span class="p">]),</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">""</span><span class="p">):</span>
            <span class="n">sync_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_variable_averages</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_variables_to_average</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_chief_queue_runner</span> <span class="o">=</span> <span class="n">queue_runner</span><span class="o">.</span><span class="n">QueueRunner</span><span class="p">(</span><span class="n">dummy_queue</span><span class="p">,</span>
                                                            <span class="p">[</span><span class="n">sync_op</span><span class="p">])</span>
      <span class="k">for</span> <span class="n">accum</span><span class="p">,</span> <span class="n">dev</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_accumulator_list</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">dev</span><span class="p">):</span>
          <span class="n">chief_init_ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
              <span class="n">accum</span><span class="o">.</span><span class="n">set_global_step</span><span class="p">(</span>
                  <span class="n">global_step</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"SetGlobalStep"</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">chief_init_op</span> <span class="o">=</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">chief_init_ops</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_gradients_applied</span> <span class="o">=</span> <span class="bp">True</span>
      <span class="k">return</span> <span class="n">train_op</span>
</code></pre></div></div>

<p>上面的流程说明了如何通过<code class="highlighter-rouge">token_queue</code>控制各个worker的计算进度。在上面代码给出的流程中，我们可以将token看做计算集群当前要求各个worker完成的任务，如果worker获得了token，那么代表着这个worker领取了token表示的<code class="highlighter-rouge">global_step</code>的任务。<code class="highlighter-rouge">token_queue</code>的更新是需要等待真实的Optimizer完成了参数的更新之后，才会向token queue中放入一定数目的新的token。</p>

<p>有一点需要注意，取token这个Operation只依赖于<code class="highlighter-rouge">train_ops</code>，而<code class="highlighter-rouge">train_ops</code>表示着在Accumulator中的apply_gradients的Operation，可以简单的把<code class="highlighter-rouge">train_ops</code>看做push的过程。这意味着，当worker完成了一轮梯度计算之后，如果<code class="highlighter-rouge">token_queue</code>中还有剩余的token，那么它将会领取一个剩余的token，利用下一个batch的数据开始一轮相同的迭代。</p>

<p>至此，<code class="highlighter-rouge">sync_replicas_optimizer</code>中最重要的，利用平均后的梯度更新模型参数的方法<code class="highlighter-rouge">apply_gradients()</code>的执行流程，应该已经比较清楚了，总结一下，比较重要的是有以下几个点：</p>
<ol>
  <li>ConditionalAccumulator的梯度聚合
ConditionalAccumulator可以聚合那些合适的梯度，stale的梯度值会被自动抛弃，且直到收集到足够的梯度之后，才能获得平均梯度。</li>
  <li>Token控制训练进度
在<a href="https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer#get_init_tokens_op">官方文档</a>中，对Token提到的关键作用在于，当worker数目少于每一轮迭代需要收集的梯度(<code class="highlighter-rouge">total_num_replicas</code>  &lt;  ` replicas_to_aggregate`)
时，训练进程能够正常迭代下去。但是除了文档提到的这个特性之外，在整个训练过程开始之前，初始的token数目将会影响到各个worker的训练进度。下面将先看一下初始token数目的设置，再分析token数目对训练进度产生影响的原因。</li>
</ol>

<h5 id="get_init_tokens_op"><code class="highlighter-rouge">get_init_tokens_op</code></h5>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">get_init_tokens_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_tokens</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradients_applied</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">:</span>
      <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span>
          <span class="s">"get_init_tokens_op() should be called after apply_gradients()."</span><span class="p">)</span>

    <span class="n">tokens_needed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replicas_to_aggregate</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_num_replicas</span>
    <span class="k">if</span> <span class="n">num_tokens</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
      <span class="n">num_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replicas_to_aggregate</span>
    <span class="k">elif</span> <span class="n">num_tokens</span> <span class="o">&lt;</span> <span class="n">tokens_needed</span><span class="p">:</span>
      <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span>
          <span class="s">"Too few tokens to finish the first step: </span><span class="si">%</span><span class="s">d (given) vs </span><span class="si">%</span><span class="s">d (needed)"</span> <span class="o">%</span>
          <span class="p">(</span><span class="n">num_tokens</span><span class="p">,</span> <span class="n">tokens_needed</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">num_tokens</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">""</span><span class="p">):</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="n">num_tokens</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">)</span>
        <span class="n">init_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sync_token_queue</span><span class="o">.</span><span class="n">enqueue_many</span><span class="p">((</span><span class="n">tokens</span><span class="p">,))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">init_tokens</span> <span class="o">=</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">no_op</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"no_init_tokens"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">init_tokens</span>

  <span class="k">def</span> <span class="nf">make_session_run_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_chief</span><span class="p">,</span> <span class="n">num_tokens</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
    <span class="s">"""Creates a hook to handle SyncReplicasHook ops such as initialization."""</span>
    <span class="k">return</span> <span class="n">_SyncReplicasOptimizerHook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_chief</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">)</span>
</code></pre></div></div>

<p>这部分的代码给出了在训练开始之前，向token_queue放入多少token的执行流程。这一部分的代码会通过hook的方式，在session创建之后，对模型参数初始化时执行。</p>

<p>在默认情况下，如果不人为指定参数，传入的参数<code class="highlighter-rouge">num_tokens</code>为<code class="highlighter-rouge">-1</code>，此时将会在<code class="highlighter-rouge">token_queue</code>中放入<code class="highlighter-rouge">replicas_to_aggregate</code>个token。在实际实验过程中，这将会导致一些不同步的问题，在issues <a href="https://github.com/tensorflow/tensorflow/issues/11753#issuecomment-377855509">#11753</a>中提到了解决方法，是将初始token数目设置为0即可。我通过实验也验证了这个方法的有效性。</p>

<p>上面的<code class="highlighter-rouge">apply_gradients</code>的执行流程，可以解释这个现象发生的原因。因为当worker完成了向参数服务器 push 梯度的过程之后，就会申请dequeue，如果此时<code class="highlighter-rouge">token_queue</code>中有token，那么worker就能取新的训练数据，依然利用刚才的模型参数再计算一轮梯度。实际上，如果各个worker算力差别很大，是可能出现K-batch-sync SGD这种训练模式的，下图很好的说明了这一点。下图的设置可以理解为<code class="highlighter-rouge">total_num_replicas=3</code>,<code class="highlighter-rouge">replicas_to_aggregate=2</code>,的情况。L2的计算速度相较于另外两个worker快很多，由于它本地迭代一轮之后，可以从queue中取出一个token，开始新的一轮计算。本地迭代两轮之后，另外两个worker还未迭代完一轮，这时，参数服务器将会利用L2的两次梯度进行平均，对参数更新之后，<code class="highlighter-rouge">global_step</code>递增1，开始全局的新一轮计算。
<img src="/img/1567651943862.png" alt="@K-batch-sync SGD" /></p>
<h3 id="参考资料">参考资料</h3>
<ol>
  <li><a href="https://github.com/tensorflow/tensorflow/issues/11753">tf.train.SyncReplicasOptimizer no synchronization among workers #11753</a></li>
  <li><a href="https://github.com/tensorflow/tensorflow/issues/9596">Synchronous distributed tensorflow training doesn’t synchronize among workers #9596</a></li>
  <li><a href="tf.train.SyncReplicasOptimizer">tf.train.SyncReplicasOptimizer</a></li>
  <li><a href="https://zhuanlan.zhihu.com/p/40342278">Optimizer in Tensorflow</a></li>
  <li><a href="https://arxiv.org/abs/1803.01113">Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD</a></li>
</ol>



                <hr>

                


                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2019/08/25/About-Python/" data-toggle="tooltip" data-placement="top" title="About Python">&larr; Previous Post</a>
                    </li>
                    
                    
                </ul>


                

                

            </div>

    <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <hr class="hidden-sm hidden-xs">
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
        				
                            
                				<a href="/tags/#深度学习" title="深度学习" rel="2">
                                    深度学习
                                </a>
                            
        				
                            
                				<a href="/tags/#分布式训练" title="分布式训练" rel="2">
                                    分布式训练
                                </a>
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
        			</div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">
                    
                        <li><a href="http://huangxuan.me">Hux Blog</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>








<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("http://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'always',
          placement: 'right',
          icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    
                    

                    <!-- add Weibo, Zhihu by Hux, add target = "_blank" to <a> by Hux -->
                    
                    


                    
                    
                    <li>
                        <a target="_blank" href="https://github.com/yytwz96">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Logging.debug 2019
                    <br>
                    Theme by <a href="http://huangxuan.me">Hux</a> |
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="91px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=huxpro&repo=huxpro.github.io&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js "></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("http://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->



<!-- Baidu Tongji -->




<!-- Image to hack wechat -->
<img src="/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
