<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Logging.debug</title>
    <description>1</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 20 Sep 2019 20:02:00 +0800</pubDate>
    <lastBuildDate>Fri, 20 Sep 2019 20:02:00 +0800</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>TensorFlow SyncReplicasOptimizer 解读</title>
        <description>&lt;h3 id=&quot;背景&quot;&gt;背景&lt;/h3&gt;
&lt;p&gt;在CNN等深度神经网络的分布式训练中，比较常用的一种训练模式是（同步的）数据并行，也就是各个计算设备分别根据各自获得的batch，前向计算获得损失，进而反向传播计算梯度。待所有计算设备完成梯度计算之后，对梯度进行平均，利用平均梯度对模型进行更新。&lt;/p&gt;

&lt;p&gt;上面这种模式只是一种逻辑架构，具体实现上，可以使用参数服务器（PS）的形式实现。在参数服务器架构中，计算设备被划分为参数服务器(ps)和worker。对于ps，顾名思义，主要是对模型的参数进行存储；而对于worker，主要的工作是完成前向和反向传播这类计算密集的运算。&lt;/p&gt;

&lt;p&gt;在（同步的）参数服务器架构中，整个流程可以分为以下几个步骤：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;worker从ps把模型参数pull到本地的memory中&lt;/li&gt;
  &lt;li&gt;worker利用模型参数完成前向计算，再完成反向传播，得到模型参数的梯度&lt;/li&gt;
  &lt;li&gt;worker将模型参数的梯度push至参数服务器，参数服务器收到所有worker的梯度之后，计算平均梯度，并对模型参数进行更新&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;之所以想到查看&lt;code class=&quot;highlighter-rouge&quot;&gt;SyncReplicasOptimizer&lt;/code&gt;的实现，是因为我在实验中，用到了这个Optimizer，最初看它的名字，想当然的认为它是一个同步的Optimizer，但是实际使用发现如果参数设置不当，可能会出现各个worker计算进度不一致，也就是没有真正的同步。另外，在实验中发现在我的实验环境下，PS架构 + &lt;code class=&quot;highlighter-rouge&quot;&gt;sync_replicas_optimizer&lt;/code&gt;不能完全发挥集群的性能，可能需要了解sync_replicas_optimizer的工作原理才能更好的发现问题所在。&lt;/p&gt;

&lt;p&gt;下面的介绍是我个人查看&lt;code class=&quot;highlighter-rouge&quot;&gt;tf.train.Sync_replicas_optimizer&lt;/code&gt;实现部分的理解。&lt;/p&gt;

&lt;h3 id=&quot;syncreplicasoptimizer解析&quot;&gt;SyncReplicasOptimizer解析&lt;/h3&gt;
&lt;h4 id=&quot;文档说明&quot;&gt;文档说明&lt;/h4&gt;
&lt;p&gt;在TensorFlow r1.14版本的&lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer&quot;&gt;文档&lt;/a&gt;中，这个Optimizer提供的API已经被弃用了，在新的API中，如果希望在分布式环境中实现同步的训练，是通过配置&lt;a href=&quot;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute&quot;&gt;Distribution Strategies&lt;/a&gt;来实现的。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This class is deprecated. For synchrononous training, please use &lt;a href=&quot;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute&quot;&gt;Distribution Strategies&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在文档中，给出了这个优化器的“同步”训练方案，如下：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;For the Parameter Server job:
      &lt;ol&gt;
        &lt;li&gt;An accumulator is created for each variable, and each replica pushes the gradients into the accumulators instead of directly applying them to the variables.&lt;/li&gt;
        &lt;li&gt;Each accumulator averages once enough gradients (replicas_to_aggregate) have been accumulated.&lt;/li&gt;
        &lt;li&gt;Apply the averaged gradients to the variables.&lt;/li&gt;
        &lt;li&gt;Only after all variables have been updated, increment the global step.&lt;/li&gt;
        &lt;li&gt;Only after step 4, pushes global_step in the token_queue, once for each worker replica. The workers can now fetch the global step, use it to update its local_step variable and start the next batch. Please note that some workers can consume multiple minibatches, while some may not consume even one. This is because each worker fetches minibatches as long as a token exists. If one worker is stuck for some reason and does not consume a token, another worker can use it.&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/li&gt;
    &lt;li&gt;For the replicas:
      &lt;blockquote&gt;
        &lt;ol&gt;
          &lt;li&gt;Start a step: fetch variables and compute gradients.&lt;/li&gt;
          &lt;li&gt;Once the gradients have been computed, push them into gradient accumulators. Each accumulator will check the staleness and drop the stale.&lt;/li&gt;
          &lt;li&gt;After pushing all the gradients, dequeue an updated value of global_step from the token queue and record that step to its local_step variable. Note that this is effectively a barrier.&lt;/li&gt;
          &lt;li&gt;Start the next batch.&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/blockquote&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;在文档中，Parameter Server job可以简单理解为参数服务器，而Replicas可以简单理解为各个worker，或者是各个GPU。可以从文档中得到这两个信息：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;参数服务器是通过为每一个variable建立一个&lt;code class=&quot;highlighter-rouge&quot;&gt;accumulator&lt;/code&gt;数据结构，进而对来自不同worker的梯度进行管理（平均）的&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;token_queue&lt;/code&gt;这个数据结构被用来保持各个worker的进度（一致）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;文档中的叙述展示了这个Optimizer的执行逻辑，但是对于TensorFlow而言，它是通过计算图完成计算的，上面的流程如何在计算图中得到体现，就需要查看&lt;code class=&quot;highlighter-rouge&quot;&gt;sync_replicas_optimizer&lt;/code&gt;的实现部分。&lt;/p&gt;
&lt;h4 id=&quot;源码部分&quot;&gt;源码部分&lt;/h4&gt;
&lt;h5 id=&quot;__init__&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt;&lt;/h5&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sync_replicas_optimizer&lt;/code&gt;是一个wrapper optimizer，也就是这个Optimizer对其他Optimizer进行包装，完成worker间梯度同步的工作，而实际梯度的计算则是交由被包装的Optimizer来完成的。这一点可以通过构造函数发现：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;replicas_to_aggregate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;total_num_replicas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;variable_averages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;variables_to_average&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;use_locking&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sync_replicas&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
               &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_num_replicas&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
				    &lt;span class=&quot;n&quot;&gt;total_num_replicas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replicas_to_aggregate&lt;/span&gt;
			    &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SyncReplicasOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;use_locking&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			    &lt;span class=&quot;n&quot;&gt;logging&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
			        &lt;span class=&quot;s&quot;&gt;&quot;SyncReplicasV2: replicas_to_aggregate=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s; total_num_replicas=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
			        &lt;span class=&quot;n&quot;&gt;replicas_to_aggregate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_num_replicas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;
			    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_replicas_to_aggregate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replicas_to_aggregate&lt;/span&gt;
			    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_gradients_applied&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
			    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_variable_averages&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variable_averages&lt;/span&gt;
			    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_variables_to_average&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variables_to_average&lt;/span&gt;
			    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_total_num_replicas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_num_replicas&lt;/span&gt;
			    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_tokens_per_step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_num_replicas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replicas_to_aggregate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_global_step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
			    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_sync_token_queue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
			    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_chief_queue_runner&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
			    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_accumulator_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在构造函数中，可以传入这么几个参数，&lt;code class=&quot;highlighter-rouge&quot;&gt;opt&lt;/code&gt;,&lt;code class=&quot;highlighter-rouge&quot;&gt;replicas_to_aggregate&lt;/code&gt;,&lt;code class=&quot;highlighter-rouge&quot;&gt;total_num_replicas&lt;/code&gt;,&lt;code class=&quot;highlighter-rouge&quot;&gt;variable_averages&lt;/code&gt;,&lt;code class=&quot;highlighter-rouge&quot;&gt;variables_to_average&lt;/code&gt;,&lt;code class=&quot;highlighter-rouge&quot;&gt;use_locking&lt;/code&gt;,&lt;code class=&quot;highlighter-rouge&quot;&gt;name&lt;/code&gt;。各个参数具体的含义可以参考&lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer#__init__&quot;&gt;官方文档&lt;/a&gt;给出的解释。需要说明的有几个点，&lt;code class=&quot;highlighter-rouge&quot;&gt;opt&lt;/code&gt;这个参数体现了wrapper的思想，它是一个&lt;code class=&quot;highlighter-rouge&quot;&gt;Optimizer&lt;/code&gt;类的一个对象，通过它来真正完成梯度的计算。&lt;code class=&quot;highlighter-rouge&quot;&gt;total_num_replicas&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;replicas_to_aggregate&lt;/code&gt;这两个参数会被用来控制同步的进度，前者表示&lt;strong&gt;集群中worker的数目&lt;/strong&gt;，而后者表示&lt;strong&gt;ps完成一次参数更新所需要收集的梯度数目&lt;/strong&gt;。在同步的训练模式下，它们两个应当是相等的，但是事实上，即使它们俩相等，也有可能导致不同步的现象出现，这与其他参数(&lt;code class=&quot;highlighter-rouge&quot;&gt;token_num&lt;/code&gt;)的设置有关系。&lt;/p&gt;

&lt;p&gt;如果前者比后者大，逻辑上意味着ps在一轮迭代中，只需要收集到部分梯度，就利用这些梯度计算平均梯度，进而对参数更新，这可能意味着这轮迭代中，计算较慢的那个节点的梯度会被丢弃&lt;em&gt;（只是一种猜测，还未实验验证）&lt;/em&gt;。这两个参数与之前提到的&lt;code class=&quot;highlighter-rouge&quot;&gt;token_queue&lt;/code&gt;的设置有关系，在后面的源码介绍将会更加详细的说明。&lt;/p&gt;

&lt;p&gt;在构造函数中，还为这个Optimizer定义了一些新的数据结构，就不详细介绍了，读完整个执行流程之后，它们的功能应该就会比较清楚。&lt;/p&gt;

&lt;h5 id=&quot;compute_gradients&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;compute_gradients&lt;/code&gt;&lt;/h5&gt;
&lt;p&gt;下面来看&lt;code class=&quot;highlighter-rouge&quot;&gt;compute_gradients&lt;/code&gt;的实现。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;compute_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	   &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compute_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;可以从源码中验证wrapper的思想，梯度的计算是通过被包装的Optimizer来实现的。&lt;/p&gt;

&lt;h5 id=&quot;apply_gradients&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;apply_gradients&lt;/code&gt;&lt;/h5&gt;
&lt;p&gt;这个方法将会构造模型参数更新部分的计算图，最后将返回一个&lt;code class=&quot;highlighter-rouge&quot;&gt;train_op&lt;/code&gt;。&lt;code class=&quot;highlighter-rouge&quot;&gt;train_op&lt;/code&gt;是通常训练过程中，client为session的fetches提供的参数之一，也就是这个Operation被执行之后，模型的参数将会完成更新，并开始下一个batch的训练。那么这也就意味着，这个方法中涉及到的计算图将会实现说明文档中的训练逻辑。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;apply_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads_and_vars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;global_step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads_and_vars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Must supply at least one variable&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;global_step&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Global step is required to check staleness&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_global_step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;global_step&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_ops&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 所有的train_op，也就是Accumulator的apply_gradients，实际上可以指push的过程
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;aggregated_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 平均后的梯度
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;var_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 变量存放的列表
&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;上面的代码负责验证传入方法的参数的合法性，并创建一些供方法内部使用的数据结构。传入的参数必须包含有&lt;code class=&quot;highlighter-rouge&quot;&gt;global_step&lt;/code&gt;，这在其他Optimizer可能不是一个必须的参数，在这里，它为必须的，因为它需要被用在检查stale gradiets的地方。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;n&quot;&gt;local_anchor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_flow_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colocate_with&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;local_anchor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_local_step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variable_scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;initial_value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;trainable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;collections&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GraphKeys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LOCAL_VARIABLES&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sync_rep_local_step&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;# _local_step的初始化Operation
&lt;/span&gt;      &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;local_step_init_op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;assign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_local_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;global_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;chief_init_ops&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;local_step_init_op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
      &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ready_for_local_init_op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variables&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;report_uninitialized_variables&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;variables&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这一部分首先在计算图上创建了一个空的Operation，之后创建了一个类的成员变量&lt;code class=&quot;highlighter-rouge&quot;&gt;_local_step&lt;/code&gt;，用来记录这个worker的计算进度。用到&lt;code class=&quot;highlighter-rouge&quot;&gt;ops.colocate_with()&lt;/code&gt;的原因是在于，在一般情况下，PS架构会通过device function等机制，将这类Operation放在worker所处的device上，而将Variables这类特殊的Operation放在PS上。&lt;code class=&quot;highlighter-rouge&quot;&gt;local_anchor&lt;/code&gt;是一个一般的Operation，因此它会被PS架构分配至worker所处的device，这样就能确保创建的&lt;code class=&quot;highlighter-rouge&quot;&gt;_local_step&lt;/code&gt;变量也能够放在worker上。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 在计算图中创建以这个优化器为名称的name_scope
&lt;/span&gt;      &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads_and_vars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 遍历计算好的每一份梯度
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;var_list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 将variables按顺序保存在一个列表里
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 下面定义的计算图的部分将与variables放在相同的device上。可以理解为都被放在PS上
&lt;/span&gt;          &lt;span class=&quot;c1&quot;&gt;# Dense gradients.
&lt;/span&gt;          &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;aggregated_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# pass-through.
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# 为每一份梯度创建一个ConditionalAccumulator
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;grad_accum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_flow_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ConditionalAccumulator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;shared_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;/grad_accum&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# 在train_ops这个集合中放入对应accumulator的apply_grad operation
&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;# 注意，这个apply_grad Operation传入了一个local_step参数，这个操Operation是把梯度放到accumulator中，如果local_step落后于global_step，这个Operation会自动终止，也就是梯度不会放入accumulator
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;train_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_accum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;local_step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_local_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# aggregrated_grad中有所有平均之后的梯度，它是从accumulator中执行take_grad的Operation
&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;# take_grad()中有一个参数，num_required，如果说accumulator中的梯度数量少于num_required，它将会阻塞。
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;aggregated_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_accum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;take_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_replicas_to_aggregate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
		    &lt;span class=&quot;c1&quot;&gt;# 稀疏梯度的处理，与上面类似
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IndexedSlices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
              &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Unknown grad type!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;grad_accum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_flow_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SparseConditionalAccumulator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shared_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;/grad_accum&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;train_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_accum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply_indexed_slices_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;local_step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_local_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;aggregated_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_accum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;take_indexed_slices_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_replicas_to_aggregate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_accumulator_list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_accum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;aggregated_grads_and_vars&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aggregated_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 最终的(平均梯度，参数)列表
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;上面的代码给出的是进行梯度聚合，最终获得对应每一个参数的平均梯度的过程。在上面这个过程中，分别为每一个Variable创建了一个&lt;code class=&quot;highlighter-rouge&quot;&gt;ConditionalAccumulator&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;ConditionalAccumulator&lt;/code&gt;是被多个Session所共享的，用于维护在这个time step下来自不同worker的梯度。ConditionalAccumulator内部实际上维护了一个time step，记录了当前集群训练的进度。在Accumulator中提供了&lt;code class=&quot;highlighter-rouge&quot;&gt;apply_grad()&lt;/code&gt;方法，这个方法需要将&lt;code class=&quot;highlighter-rouge&quot;&gt;local_step&lt;/code&gt;作为一个参数传入，返回一个Operation，如果&lt;code class=&quot;highlighter-rouge&quot;&gt;local_step&lt;/code&gt;小于内部的time step，那这个Operation就不会被执行。而&lt;code class=&quot;highlighter-rouge&quot;&gt;take_grad()&lt;/code&gt;方法在Accumulator内部收集的gradient数目少于&lt;code class=&quot;highlighter-rouge&quot;&gt;num_required&lt;/code&gt;参数指出的数目时，就会阻塞，它被调用完之后，Accumulator内部的gradient计数器会清零，同时内部的time step会递增1个单位。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      &lt;span class=&quot;c1&quot;&gt;# sync_op will be assigned to the same device as the global step.
&lt;/span&gt;      &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;update_op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aggregated_grads_and_vars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                              &lt;span class=&quot;n&quot;&gt;global_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 真正完成参数更新的地方，执行之后，global_step会+1
&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;# Create token queue.
&lt;/span&gt;      &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sync_token_queue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;data_flow_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FIFOQueue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                    &lt;span class=&quot;n&quot;&gt;global_step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                    &lt;span class=&quot;n&quot;&gt;shapes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
                                    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sync_token_q&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                    &lt;span class=&quot;n&quot;&gt;shared_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sync_token_q&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_sync_token_queue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sync_token_queue&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# dummy_queue is passed to the queue runner. Don't use the real queues
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# because the queue runner doesn't automatically reopen it once it
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# closed queues in PS devices.
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;dummy_queue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;data_flow_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FIFOQueue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                    &lt;span class=&quot;n&quot;&gt;types_pb2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DT_INT32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                    &lt;span class=&quot;n&quot;&gt;shapes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
                                    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dummy_queue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                    &lt;span class=&quot;n&quot;&gt;shared_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dummy_queue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Replicas have to wait until they can get a token from the token queue.
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;control_dependencies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_ops&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sync_token_queue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dequeue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 这个Operation只依赖与train_ops，而不依赖于update_op
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;train_op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;assign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_local_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 更新_local_step的值为集群所要求的step
&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;control_dependencies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
          &lt;span class=&quot;c1&quot;&gt;# Sync_op needs to insert tokens to the token queue at the end of the
&lt;/span&gt;          &lt;span class=&quot;c1&quot;&gt;# step so the replicas can fetch them to start the next step.
&lt;/span&gt;          &lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_tokens_per_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;global_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;sync_op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sync_token_queue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enqueue_many&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,))&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 将新的toekns放入队列中，tokens是新的global_step
&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_variable_averages&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;control_dependencies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sync_op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;sync_op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_variable_averages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_variables_to_average&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_chief_queue_runner&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queue_runner&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QueueRunner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dummy_queue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                                            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sync_op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dev&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_accumulator_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;chief_init_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;accum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_global_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                  &lt;span class=&quot;n&quot;&gt;global_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;SetGlobalStep&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
      &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chief_init_op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_flow_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chief_init_ops&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
      &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_gradients_applied&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_op&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;上面的流程说明了如何通过&lt;code class=&quot;highlighter-rouge&quot;&gt;token_queue&lt;/code&gt;控制各个worker的计算进度。在上面代码给出的流程中，我们可以将token看做计算集群当前要求各个worker完成的任务，如果worker获得了token，那么代表着这个worker领取了token表示的&lt;code class=&quot;highlighter-rouge&quot;&gt;global_step&lt;/code&gt;的任务。&lt;code class=&quot;highlighter-rouge&quot;&gt;token_queue&lt;/code&gt;的更新是需要等待真实的Optimizer完成了参数的更新之后，才会向token queue中放入一定数目的新的token。&lt;/p&gt;

&lt;p&gt;有一点需要注意，取token这个Operation只依赖于&lt;code class=&quot;highlighter-rouge&quot;&gt;train_ops&lt;/code&gt;，而&lt;code class=&quot;highlighter-rouge&quot;&gt;train_ops&lt;/code&gt;表示着在Accumulator中的apply_gradients的Operation，可以简单的把&lt;code class=&quot;highlighter-rouge&quot;&gt;train_ops&lt;/code&gt;看做push的过程。这意味着，当worker完成了一轮梯度计算之后，如果&lt;code class=&quot;highlighter-rouge&quot;&gt;token_queue&lt;/code&gt;中还有剩余的token，那么它将会领取一个剩余的token，利用下一个batch的数据开始一轮相同的迭代。&lt;/p&gt;

&lt;p&gt;至此，&lt;code class=&quot;highlighter-rouge&quot;&gt;sync_replicas_optimizer&lt;/code&gt;中最重要的，利用平均后的梯度更新模型参数的方法&lt;code class=&quot;highlighter-rouge&quot;&gt;apply_gradients()&lt;/code&gt;的执行流程，应该已经比较清楚了，总结一下，比较重要的是有以下几个点：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;ConditionalAccumulator的梯度聚合
ConditionalAccumulator可以聚合那些合适的梯度，stale的梯度值会被自动抛弃，且直到收集到足够的梯度之后，才能获得平均梯度。&lt;/li&gt;
  &lt;li&gt;Token控制训练进度
在&lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer#get_init_tokens_op&quot;&gt;官方文档&lt;/a&gt;中，对Token提到的关键作用在于，当worker数目少于每一轮迭代需要收集的梯度(&lt;code class=&quot;highlighter-rouge&quot;&gt;total_num_replicas&lt;/code&gt;  &amp;lt;  ` replicas_to_aggregate`)
时，训练进程能够正常迭代下去。但是除了文档提到的这个特性之外，在整个训练过程开始之前，初始的token数目将会影响到各个worker的训练进度。下面将先看一下初始token数目的设置，再分析token数目对训练进度产生影响的原因。&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;get_init_tokens_op&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;get_init_tokens_op&lt;/code&gt;&lt;/h5&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_init_tokens_op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_tokens&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_gradients_applied&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&quot;get_init_tokens_op() should be called after apply_gradients().&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;tokens_needed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_replicas_to_aggregate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_total_num_replicas&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;num_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_replicas_to_aggregate&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokens_needed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&quot;Too few tokens to finish the first step: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d (given) vs &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d (needed)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokens_needed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_global_step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_global_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;init_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_sync_token_queue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enqueue_many&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;init_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_flow_ops&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;no_init_tokens&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init_tokens&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;make_session_run_hook&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is_chief&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_tokens&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Creates a hook to handle SyncReplicasHook ops such as initialization.&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_SyncReplicasOptimizerHook&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is_chief&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这部分的代码给出了在训练开始之前，向token_queue放入多少token的执行流程。这一部分的代码会通过hook的方式，在session创建之后，对模型参数初始化时执行。&lt;/p&gt;

&lt;p&gt;在默认情况下，如果不人为指定参数，传入的参数&lt;code class=&quot;highlighter-rouge&quot;&gt;num_tokens&lt;/code&gt;为&lt;code class=&quot;highlighter-rouge&quot;&gt;-1&lt;/code&gt;，此时将会在&lt;code class=&quot;highlighter-rouge&quot;&gt;token_queue&lt;/code&gt;中放入&lt;code class=&quot;highlighter-rouge&quot;&gt;replicas_to_aggregate&lt;/code&gt;个token。在实际实验过程中，这将会导致一些不同步的问题，在issues &lt;a href=&quot;https://github.com/tensorflow/tensorflow/issues/11753#issuecomment-377855509&quot;&gt;#11753&lt;/a&gt;中提到了解决方法，是将初始token数目设置为0即可。我通过实验也验证了这个方法的有效性。&lt;/p&gt;

&lt;p&gt;上面的&lt;code class=&quot;highlighter-rouge&quot;&gt;apply_gradients&lt;/code&gt;的执行流程，可以解释这个现象发生的原因。因为当worker完成了向参数服务器 push 梯度的过程之后，就会申请dequeue，如果此时&lt;code class=&quot;highlighter-rouge&quot;&gt;token_queue&lt;/code&gt;中有token，那么worker就能取新的训练数据，依然利用刚才的模型参数再计算一轮梯度。实际上，如果各个worker算力差别很大，是可能出现K-batch-sync SGD这种训练模式的，下图很好的说明了这一点。下图的设置可以理解为&lt;code class=&quot;highlighter-rouge&quot;&gt;total_num_replicas=3&lt;/code&gt;,&lt;code class=&quot;highlighter-rouge&quot;&gt;replicas_to_aggregate=2&lt;/code&gt;,的情况。L2的计算速度相较于另外两个worker快很多，由于它本地迭代一轮之后，可以从queue中取出一个token，开始新的一轮计算。本地迭代两轮之后，另外两个worker还未迭代完一轮，这时，参数服务器将会利用L2的两次梯度进行平均，对参数更新之后，&lt;code class=&quot;highlighter-rouge&quot;&gt;global_step&lt;/code&gt;递增1，开始全局的新一轮计算。
&lt;img src=&quot;/img/1567651943862.png&quot; alt=&quot;@K-batch-sync SGD&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;参考资料&quot;&gt;参考资料&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tensorflow/tensorflow/issues/11753&quot;&gt;tf.train.SyncReplicasOptimizer no synchronization among workers #11753&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tensorflow/tensorflow/issues/9596&quot;&gt;Synchronous distributed tensorflow training doesn’t synchronize among workers #9596&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;tf.train.SyncReplicasOptimizer&quot;&gt;tf.train.SyncReplicasOptimizer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/40342278&quot;&gt;Optimizer in Tensorflow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1803.01113&quot;&gt;Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Thu, 05 Sep 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/09/05/TensorFlow-SyncReplicasOptimizer/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/09/05/TensorFlow-SyncReplicasOptimizer/</guid>
        
        <category>TensorFlow</category>
        
        <category>深度学习</category>
        
        <category>分布式训练</category>
        
        <category>参数服务器</category>
        
        
      </item>
    
      <item>
        <title>About Python</title>
        <description>&lt;h1 id=&quot;pyc文件&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pyc&lt;/code&gt;文件&lt;/h1&gt;
&lt;p&gt;Python是一门解释型的语言，但是在运行之前，需要将&lt;code class=&quot;highlighter-rouge&quot;&gt;.py&lt;/code&gt;文件中的源码编译为&lt;code class=&quot;highlighter-rouge&quot;&gt;.pyc&lt;/code&gt;文件，&lt;code class=&quot;highlighter-rouge&quot;&gt;.pyc&lt;/code&gt;文件存放的是源代码在Python解释器运行之后编译得到的byte code。Python虚拟机之后再执行这些字节码。
根据这个说明，Python程序在执行之前都会生成&lt;code class=&quot;highlighter-rouge&quot;&gt;.pyc&lt;/code&gt;文件，因为只有byte code才能够被相应的虚拟机所执行。但是，实际上，并不完全正确。
假设现在有两个Python脚本文件&lt;code class=&quot;highlighter-rouge&quot;&gt;abc.py&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;xyz.py&lt;/code&gt;，其中&lt;code class=&quot;highlighter-rouge&quot;&gt;abc.py&lt;/code&gt;中将xyz作为一个模块导入其中，换句话说，&lt;code class=&quot;highlighter-rouge&quot;&gt;abc.py&lt;/code&gt;中有&lt;code class=&quot;highlighter-rouge&quot;&gt;import xyz&lt;/code&gt;语句。如果执行&lt;code class=&quot;highlighter-rouge&quot;&gt;python abc.py&lt;/code&gt;，那么只会有&lt;code class=&quot;highlighter-rouge&quot;&gt;xyz.pyc&lt;/code&gt;文件。
这个例子说明了，运行某个脚本时，python解释器并不会为这个脚本生成&lt;code class=&quot;highlighter-rouge&quot;&gt;.pyc&lt;/code&gt;文件，只会为这个脚本&lt;code class=&quot;highlighter-rouge&quot;&gt;import&lt;/code&gt;的一些模块生成&lt;code class=&quot;highlighter-rouge&quot;&gt;pyc&lt;/code&gt;文件。其实这种文件存在的主要原因是为了加速&lt;code class=&quot;highlighter-rouge&quot;&gt;Python&lt;/code&gt;程序的编译运行速度，当导入的模块没有发生变化的时候，就不需要再重新生成&lt;code class=&quot;highlighter-rouge&quot;&gt;pyc&lt;/code&gt;文件了。
在&lt;code class=&quot;highlighter-rouge&quot;&gt;Python2&lt;/code&gt;中，&lt;code class=&quot;highlighter-rouge&quot;&gt;pyc&lt;/code&gt;文件通常生成之后会存放在与&lt;code class=&quot;highlighter-rouge&quot;&gt;py&lt;/code&gt;文件相同的目录下。在&lt;code class=&quot;highlighter-rouge&quot;&gt;Python3&lt;/code&gt;中，这些文件会放在&lt;code class=&quot;highlighter-rouge&quot;&gt;__pycache__&lt;/code&gt;目录中。&lt;/p&gt;
&lt;h1 id=&quot;python是解释型语言&quot;&gt;Python是“解释型”语言？&lt;/h1&gt;
&lt;p&gt;在查找关于&lt;code class=&quot;highlighter-rouge&quot;&gt;pyc&lt;/code&gt;文件的相关内容的时候，关注到有一些关于Python是否是解释型语言的讨论。将讨论的内容总结如下：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Python是一个语言规范
  通常说Python是一门编程语言，但是网上的资料给出的更加确切的论述是，Python是一种语言规范，我们写Python代码的时候，都是遵循着Python的语言规范形成&lt;code class=&quot;highlighter-rouge&quot;&gt;py&lt;/code&gt;文件。&lt;/li&gt;
  &lt;li&gt;Python有多种实现
  我理解的Python的实现的含义就是，Python的解释器将Python解释为某种字节码，并且在某种特定要求的机器上运行的一种特定环境。
  Python有多种实现，这些实现将Python脚本编译为不同字节码，并交由不同的虚拟环境进行解释执行。举几个常见的例子：
    &lt;ul&gt;
      &lt;li&gt;CPython：将Python脚本编译为CPython bytecode，然后解释执行&lt;/li&gt;
      &lt;li&gt;Jython：将Python脚本编译为JVML的bytecode，之后交由JVM进行解释执行&lt;/li&gt;
      &lt;li&gt;IronPython：首先将Python脚本解释为CIL bytecode，之后这个字节码如何执行取决于具体的环境，在.NET, GNU Portable.NET 以及 Novell Mono将会将这些字节码编译为相应的机器码执行。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;任何一门语言都不能简单的确定是否是&lt;strong&gt;解释型语言&lt;/strong&gt;
从上面的分析可以看出，一门语言实际上只定义了一种语言规范，并没有指出具体的实现方式。有些实现方式可能将源代码编译为字节码，并交由虚拟机解释执行，有些实现方式可能将源代码编译成为机器码执行。因此，讨论一门语言是解释型还是编译型的语言，是要基于某个语言的具体实现上进行讨论的。
    &lt;h1 id=&quot;参考资料&quot;&gt;参考资料&lt;/h1&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;http://effbot.org/pyfaq/how-do-i-create-a-pyc-file.htm&quot;&gt;How do I create a .pyc file?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/2998215/if-python-is-interpreted-what-are-pyc-files&quot;&gt;If Python is interpreted, what are .pyc files?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/What-are-Python-pyc-files-and-how-are-they-used&quot;&gt;What are Python .pyc files and how are they used?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/5149832/where-are-the-pyc-files&quot;&gt;where are the .pyc files?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/16869024/what-is-pycache&quot;&gt;What is &lt;strong&gt;pycache&lt;/strong&gt;?&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 25 Aug 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/08/25/About-Python/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/08/25/About-Python/</guid>
        
        <category>Python</category>
        
        <category>编程语言</category>
        
        
      </item>
    
      <item>
        <title>文章汇总</title>
        <description>&lt;h1 id=&quot;文章汇总&quot;&gt;文章汇总&lt;/h1&gt;
&lt;p&gt;在这里，我将近期读过的一些文章做一个汇总与梳理。之后这篇汇总涉及到的篇目将随着阅读的深入不断的增加。
这里包括的文章大多是与分布式深度神经网络训练相关的，不过这个领域包括许多细分的方向，各篇文章侧重的重点有所不同，当然不同方向的文章的关注点也有交集。为了更好的整理这个目录性质的文章汇总，我依据个人理解粗略对文章进行分类，并放入相应的类别下。某些文章如果与多个主题相关，它们将会在多个类别下出现。&lt;/p&gt;

&lt;p&gt;[TOC]&lt;/p&gt;

&lt;h2 id=&quot;综述&quot;&gt;综述&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;论文名称&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表刊物&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表时间&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1802.09941&quot;&gt;Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-02&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1810.11787&quot;&gt;A Hitchhiker’s Guide On Distributed Training of Deep Neural Networks&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-10&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;large-batch-training&quot;&gt;Large Batch Training&lt;/h2&gt;
&lt;h3 id=&quot;系统整体设计&quot;&gt;系统整体设计&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;论文名称&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表刊物&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表时间&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.02677&quot;&gt;Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2017-06&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3225069&quot;&gt;ImageNet Training in Minutes&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ICPP 2018&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.04325&quot;&gt;Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2017-11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1807.11205&quot;&gt;Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-07&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1902.06855&quot;&gt;Optimizing Network Performance for Distributed DNN Training on GPU Clusters: ImageNet/AlexNet Training in 1.5 Minutes&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2019-02&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1810.01993&quot;&gt;Exascale Deep Learning for Climate Analytics&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;SC 19&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Speeding up ImageNet Training on Supercomputers&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;SysML 2018&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1604.00981&quot;&gt;Revisiting distributed synchronous SGD&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ICLR 2016&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2016-04&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;算法分析与优化&quot;&gt;算法分析与优化&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;论文名称&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表刊物&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表时间&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1708.03888&quot;&gt;Large Batch Training of Convolutional Networks&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2017-08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1705.08741&quot;&gt;Train longer, generalize better: closing the generalization gap in large batch training of neural networks&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;NIPS 2017&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2017-05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1609.04836&quot;&gt;On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ICLR 2017&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2016-09&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1904.00962&quot;&gt;Reducing BERT Pre-Training Time from 3 Days to 76 Minutes&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2019-04&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1711.00489&quot;&gt;Don’t Decay the Learning Rate, Increase the Batch Size&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ICLR 2018&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2017-11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1810.01021&quot;&gt;Large Batch Size Training of Neural Networks with Adversarial Training and Second-Order Information&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Augment your batch: better training with larger batches&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2019-01&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;通信拓扑&quot;&gt;通信拓扑&lt;/h2&gt;
&lt;h3 id=&quot;ring-allreduce&quot;&gt;Ring AllReduce&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;论文名称&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表刊物&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表时间&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1802.05799&quot;&gt;Horovod: fast and easy distributed deep learning in TensorFlow&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-02&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;parameter-server&quot;&gt;Parameter Server&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;论文名称&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表刊物&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表时间&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1801.09805&quot;&gt;Parameter Box: High Performance Parameter Servers for Efficient Distributed Deep Neural Network Training&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7678-bml-a-high-performance-low-cost-gradient-synchronization-algorithm-for-dml-training.pdf&quot;&gt;BML: A High-performance, Low-cost Gradient Synchronization Algorithm for DML Training&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;NIPS 2018&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://doi.acm.org/10.1145/2901318.2901323&quot;&gt;GeePS: Scalable deep learning on distributed GPUs with a GPU-specialized parameter server&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;EuroSys 2016&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2016-04&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf&quot;&gt;Large Scale Distributed Deep Networks&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;NIPS 2012&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2012&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1905.03960&quot;&gt;Priority-based Parameter Propagation for Distributed DNN Training&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;SysML 2019&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2019-05&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;其他&quot;&gt;其他&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;论文名称&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表刊物&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表时间&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1808.02621&quot;&gt;Parallax: Sparsity-aware Data Parallel Training of Deep Neural Networks&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;EuroSys 2019&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://link.springer.com/10.1007/978-3-540-24685-5_1&quot;&gt;Optimization of Collective Reduction Operations&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ICCS 2004&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2004-06&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1801.03855&quot;&gt;MXNET-MPI: Embedding MPI parallelism in Parameter Server Task Model for scaling Deep Learning&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Proceedings of ACM Conference&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1804.05839&quot;&gt;BigDL: A Distributed Deep Learning Framework for Big Data&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-04&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Project Adam: Building an Efficient and Scalable Deep Learning Training System&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;OSDI 2014&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2014&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;RedSync: Reducing synchronization bandwidth for distributed deep learning training system&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;JPDC 2019&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;并行模式&quot;&gt;并行模式&lt;/h2&gt;
&lt;h3 id=&quot;数据并行&quot;&gt;数据并行&lt;/h3&gt;
&lt;h3 id=&quot;模型并行&quot;&gt;模型并行&lt;/h3&gt;
&lt;h3 id=&quot;流水线训练&quot;&gt;流水线训练&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;论文名称&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表刊物&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表时间&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1806.03377&quot;&gt;PipeDream: Fast and Efficient Pipeline Parallel DNN Training&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-06&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1811.06965&quot;&gt;GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-11&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;混合并行&quot;&gt;混合并行&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;论文名称&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表刊物&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表时间&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1404.5997&quot;&gt;One weird trick for parallelizing convolutional neural networks&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2014-04&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Beyond Data and Model Parallelism for Deep Neural Networks&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;SysML 2019&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2019&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;通信量化压缩&quot;&gt;通信量化&amp;amp;压缩&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;论文名称&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表刊物&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表时间&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;AdaComp: Adaptive Residual Gradient Compression for Data-Parallel Distributed Training&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;AAAI 2018&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://ieeexplore.ieee.org/document/7835789/&quot;&gt;Communication Quantization for Data-Parallel Training of Deep Neural Networks&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MLHPC&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2016-11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://ieeexplore.ieee.org/document/7835789/&quot;&gt;Communication Quantization for Data-Parallel Training of Deep Neural Networks&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MLHPC&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2016-11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning.pdf&quot;&gt;TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;NIPS 2017&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2017&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;RedSync: Reducing synchronization bandwidth for distributed deep learning training system&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;JPDC 2019&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;优化算法&quot;&gt;优化算法&lt;/h2&gt;
&lt;h3 id=&quot;异步算法&quot;&gt;异步算法&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;论文名称&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表刊物&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表时间&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1803.01113&quot;&gt;Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;AISTATS 2018&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-03&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;模型平均&quot;&gt;模型平均&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;论文名称&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表刊物&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表时间&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1808.07217&quot;&gt;Don’t Use Large Mini-Batches, Use Local SGD&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?doid=3339186.3339203&quot;&gt;Reducing global reductions in large-scale distributed training&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ICPP 2019&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2019-08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1708.01012&quot;&gt;On the convergence properties of a $K$-step averaging stochastic gradient descent algorithm for nonconvex optimization&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2017-08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://ocs.aaai.org/ojs/index.php/AAAI/article/view/4465&quot;&gt;Scalable Distributed DL Training: Batching Communication and Computation&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;AAAI 2019&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2019&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1805.09767&quot;&gt;Local SGD Converges Fast and Communicates Little&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1807.06629&quot;&gt;Parallel Restarted SGD with Faster Convergence and Less Communication: Demystifying Why Model Averaging Works for Deep Learning&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;AAAI 2019&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-07&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;A&lt;a href=&quot;http://arxiv.org/abs/1810.08313&quot;&gt;daptive Communication Strategies to Achieve the Best Error-Runtime Trade-off in Local-Update SGD&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;SysML 2019&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7117-can-decentralized-algorithms-outperform-centralized-algorithms-a-case-study-for-decentralized-parallel-stochastic-gradient-descent.pdf&quot;&gt;Can Decentralized Algorithms OutperformCentralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;NIPS 2017&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2017&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1507.01239&quot;&gt;Experiments on Parallel Training of Deep Neural Network using Model Averaging&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2015-07&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1808.07576&quot;&gt;Cooperative SGD: A Unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-08&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;其他优化&quot;&gt;其他优化&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;论文名称&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表刊物&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表时间&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1806.02508&quot;&gt;Fast Distributed Deep Learning via Worker-adaptive Batch Sizing&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2018-06&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Dynamic Mini-batch SGD for Elastic Distributed Training: Learning in the Limbo of Resources&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2019-04&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;经典文章&quot;&gt;经典文章&lt;/h2&gt;
&lt;h3 id=&quot;网络模型&quot;&gt;网络模型&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;论文名称&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表刊物&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表时间&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1512.03385&quot;&gt;Deep Residual Learning for Image Recognition&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CVPR 2016&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2015-12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1602.02410&quot;&gt;Exploring the Limits of Language Modeling&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2016-02&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?doid=3098997.3065386&quot;&gt;ImageNet classification with deep convolutional neural networks&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;NIPS 2012&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2012&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1602.05629&quot;&gt;Communication-Efficient Learning of Deep Networks from Decentralized Data&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;AISTATS 2017&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2016-02&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;数据集&quot;&gt;数据集&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;论文名称&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表刊物&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;发表时间&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1312.3005&quot;&gt;One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;arXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2013-12&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        <pubDate>Thu, 22 Aug 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/08/22/Paper-Index/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/08/22/Paper-Index/</guid>
        
        <category>深度学习</category>
        
        <category>分布式训练</category>
        
        <category>文章总结</category>
        
        
      </item>
    
  </channel>
</rss>
